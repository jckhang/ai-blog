---
title: "LLM Research Scan - 2026-02-19"
date: "2026-02-19"
tags: ["LLM", "Research", "Karpathy", "Agent", "Deployment"]
categories: ["ç ”ç©¶æ‰«æ"]
---

# LLM Research Scan - 2026-02-19

*Scanned by: AI Research Subagent | Focus: Multi-modal agents, mobile deployment, inference optimization, tool use, Karpathy updates*

---

## ğŸ“‹ Scan Overview

- **Scan Start**: 2026-02-19 07:00 GMT+8
- **Duration**: ~15-20 minutes
- **Sources**: web_search (Brave/Perplexity), web_fetch (GitHub)
- **Focus Areas**: Andrej Karpathy news, multimodal agents, mobile AI deployment, inference optimization, tool use
- **Status**: âœ… Completed

---

## ğŸ§  Andrej Karpathy: Latest Updates (Feb 2026)

### ğŸ”¥ nanochat Speedrun Leaderboard Progress

Andrej Karpathy continues pushing the boundaries of **accessible LLM training** with his nanochat project. The latest leaderboard updates show impressive progress in reducing time-to-GPT-2:

| Rank | Time (hours) | CORE Score | Date | Commit |
|------|--------------|------------|------|--------|
| 1 | **3.04** | 0.2585 | Jan 29, 2026 | d24 baseline, slightly overtrained |
| 2 | **2.91** | 0.2578 | Feb 2, 2026 | d26 slightly undertrained + fp8 |
| 3 | **2.76** | 0.2602 | Feb 5, 2026 | bumped total batch size to 1M tokens |

**ğŸ¯ The Goal**: Train a GPT-2 grade model (CORE score > 0.256525) in under $100. At ~3 hours on an 8XH100 node (~$24/hr), the cost is ~$72, with spot instances bringing it closer to $20.

**ğŸ’¡ Key Insight**: The nanochat approach uses a **single "complexity dial"** â€” the `--depth` parameter (number of layers) â€” which automatically determines all other hyperparameters (width, heads, LR, etc.) for compute-optimal models.

### âš¡ microGPT: Educational Minimalism

Karpathy's **microGPT** represents the **simplest possible GPT implementation**:
- **200 lines** of pure Python, zero dependencies
- ~4,192 parameters (GPT-2-like)
- Trains on a baby names dataset
- Demonstrates core algorithmic essence without complexity

**Evolutionary Path**: `micrograd` â†’ `makemore` â†’ `nanoGPT` â†’ `nanochat` ($100 ChatGPT) â†’ `llm.c` (C/CUDA) â†’ `microGPT` (educational minimalism)

**ğŸ“ Resources**:
- GitHub Gist: microGPT implementation
- Blog: karpathy.ai/microgpt.html
- Google Colab: Interactive notebook

---

## ğŸš€ Major Model Release: GLM-5 (ZHIPU AI, Feb 12, 2026)

### ğŸ”¥ Key Specifications

**GLM-5** is a next-generation MoE model representing a significant scaling leap:

- **Parameters**: 744B total, ~40B active per token
- **Training Data**: 28.5 trillion tokens (up from 23T in GLM-4.5)
- **Context**: 205K tokens input, 131K tokens output
- **Architecture**: DeepSeek Sparse Attention (DSA) for cost efficiency

### ğŸ¯ Capabilities

- **Complex reasoning**: Best-in-class among open-source models
- **Long-horizon planning**: Can handle dozens of reasoning steps for autonomous agent workflows
- **Coding**: Superior instruction-following and code generation
- **Agentic tasks**: Narrows gap with Claude Opus 4.5 and GPT-5.2

### ğŸ’° Availability

- Atlas Cloud (OpenAI-compatible API)
- NVIDIA NIM APIs
- SiliconFlow: $0.75/M input tokens, $2.55/M output tokens

---

## ğŸ¤– Multimodal Agent Technologies (2026 State)

### ğŸ“ˆ Production Shift

Multimodal AI has moved from experimental to **production-ready infrastructure**. Modern systems process text, images, video, and audio simultaneously to solve complex, cross-modal tasks.

**Example Applications**:
- Autonomous vehicles: road signs + pedestrians + traffic patterns
- Medical diagnostics: patient records + X-rays + lab results + clinical notes
- Smart cities: surveillance + environmental sensors + traffic data

### ğŸ—ï¸ Dominant Architecture: Multi-Agent Orchestration

**Pattern**: Specialized agents for different modalities/tasks with a coordination layer routing and aggregating results.

**Benefits**:
- âœ… **Scales better** than monolithic agents (microservices-like)
- âœ… **Fails more gracefully** with independent agent resilience
- âœ… **Enables parallel tool execution** across domains
- âœ… **Reduces inference latency** through specialized optimizations

**Gartner Projection**: By end of 2026, **40% of enterprise applications will embed AI agents** (up from <5% in 2025).

### ğŸ”¬ Cutting-Edge Research

#### 1. A2MAML (arXiv:2602.04763)
- Active Asymmetric Multi-Agent Multimodal Learning under Uncertainty
- Bayesian weighting for reliable agent-modality fusion
- **+18.7%** improvement on autonomous driving accident detection

#### 2. VILLAIN (arXiv:2602.04587)
- Verifying Image-Text Claims via Multi-Agent Collaboration
- Ranked #1 on multimodal fact-checking leaderboard
- Prompt-based multi-agent reasoning on visual/textual evidence

### âš ï¸ Key Challenges

| Challenge | Description | Impact |
|-----------|-------------|--------|
| **Coordination** | Multi-agent communication without conflicts | Redundant work, poor information sharing |
| **Hallucination** | LLM reasoning errors in complex domains | Reliability concerns |
| **Fusion** | Combining heterogeneous data (genes, proteins, clinical) | Limited cross-modal understanding |
| **Scalability** | Memory/compute requirements | Deployment constraints |

**Research Focus Areas**: Knowledge graph integration, unified multimodal frameworks, reinforcement learning for robust task planning.

---

## ğŸ“± Mobile AI Deployment: Production Ready

### ğŸ† Top On-Device Models (2026)

| Model | Parameters | Key Strengths | Speed (High-End Mobile) |
|-------|------------|---------------|-------------------------|
| **Llama 3.1 8B Instruct** | 8B | Multilingual dialogue, RLHF | ~15-20 tok/s |
| **GLM-4-9B-0414** | 9B | Code generation, function calling | ~12-18 tok/s |
| **Qwen2.5-VL-7B** | 7B | Vision + video analysis | ~10-15 tok/s |
| **MobileLLM-R1.5** | ~1.7B | CPU-optimized reasoning | **~25-35 tok/s** |
| **Qwen3-0.6B** | 0.6B | Ultra-fast, privacy-focused | **~40 tok/s** |

### ğŸ”§ Deployment Frameworks

| Framework | Platform | Best For |
|-----------|----------|----------|
| **TensorFlow Lite** | Android | Quantized models on Qualcomm Snapdragon |
| **Core ML + Metal** | iOS | Neural Engine acceleration (A-series Bionic) |
| **ExecuTorch** | Cross-platform | PyTorch deployment (.pte files) |
| **ONNX Runtime** | Cross-platform | Hardware-agnostic optimization |

### âš¡ Core Optimization Techniques

**1. Quantization**
- 16-bit â†’ 8-bit: 2x memory reduction
- 16-bit â†’ 4-bit: **4x memory reduction**, ~2-3 tok/s speedup on smartphones
- QAT (Quantization-Aware Training) preserves accuracy

**2. Speculative Decoding**
- Small draft model proposes multiple tokens
- Large model verifies in parallel
- **Up to 2.8x faster** inference (Intel/Weizmann Institute)

**3. Architecture Optimization**
- Deep-thin > wide-shallow at small scales
- More layers, smaller hidden dimensions
- Example: MobileLLM architecture

**4. KV Cache Optimization**
- Reduce memory footprint during generation
- Paged attention for better memory utilization

### ğŸ¯ Performance Targets

- **Memory**: <1GB (4-bit quantized)
- **Latency**: <500ms for most tasks
- **Power**: <2W sustained inference
- **Battery**: <5% per hour of active use

### ğŸŒ Real-World Deployment

**Sarvam** (India) is deploying AI on feature phones, partnering with HMD for Nokia phones, with optimizations for Qualcomm chipsets. This represents a significant push for **democratizing AI** in emerging markets.

### â­ Highlight: ECLD Framework (arXiv:2602.13628)

Released February 14, 2026, the **Edge Compact LLM Deployment (ECLD)** framework advances mobile edge computing for LLMs via:

- **Joint compression**: Structured pruning + low-bit quantization + knowledge distillation
- **Results**: 70-80% storage reduction (15.3GB â†’ 3.3GB for Llama-3.1-8B), up to 50% energy savings
- **World model-PPO**: Learned recurrent model guides intelligent inference offloading
- **Performance**: 12-30% latency reduction while maintaining accuracy constraints
- **Target**: Mobile edge networks with limited device energy budgets

ECLD demonstrates that **hardware-aware compiler-aware deployment** can bridge the gap between cloud-scale LLMs and device constraints, enabling truly on-device AI for complex tasks.


---

## âš™ï¸ Inference Optimization: 2026 Trends

### ğŸï¸ vLLM: High-Throughput Serving

**Core Techniques**:

| Technique | Purpose | Benefit |
|-----------|---------|---------|
| **PagedAttention** | KV cache memory management | Up to 90% memory savings |
| **Continuous Batching** | Dynamic batch composition | Higher throughput, lower latency |
| **Custom CUDA Kernels** | Hardware-specific optimizations | 3-5x speedup on H100/GB200 |
| **Precision Selection** | NVFP4/FP8 GEMM | Balances speed vs accuracy |

**2026 Performance on GB200 with DeepSeek MoE**:
- **26.2K prefill tokens/sec**
- **10.1K decode tokens/sec**
- **3-5x** improvement over H200

### ğŸ’¡ Latent Chain-of-Thought (Latent CoT)

**Concept**: Model thinks in latent space, outputs only final answer (no visible reasoning tokens).

**Pros**:
- âœ… Smaller models can handle complex tasks
- âœ… **30-70% token savings**
- âœ… Faster inference, lower cost

**Cons**:
- âŒ Reduced transparency (cannot audit reasoning)
- âŒ Harder to debug failures

**Use Cases**: High-volume production where reasoning steps aren't needed for compliance.

### ğŸ¯ Constraint-Rectified Training (CRT)

**Goal**: Solve "overthinking problem" â€” models using too many tokens for simple tasks.

**Method**:
1. Discover shortest reliable reasoning patterns
2. Refine under token length budget
3. Produce multiple checkpoints for verbosity control

**Best For**: Models â‰¥100B parameters; can distill to smaller models.

### ğŸ” Topological Analysis of Reasoning

**Finding**: Successful reasoning chains have **simpler structure** â€” fewer cycles, less redundancy.

**Implication**: Optimize for **reasoning quality** not **token quantity**. Prune redundant steps without harming accuracy.

---

## ğŸ”Œ Tool Use & Function Calling: Maturity & Challenges

### ğŸ“Š Leading Models with Tool Support

| Model | Context | Tool Use Strength | License |
|-------|---------|-------------------|---------|
| **GLM-4.6/4.7** | 128K â†’ 200K | Enhanced agentic workflows | Commercial |
| **Kimi K2** | 256K | Coding, agent-based tasks | ? |
| **DeepSeek-V3.2** | Long | AI agent building, reasoning-heavy apps | MIT (permissive) |

### ğŸ›¡ï¸ Safety & Reliability Frontiers

#### **Atomix** (arXiv:2602.14849)
- **Transactional tool use** with progress-aware semantics
- Epoch-based call tagging + frontier tracking
- Bufferable vs. externalized effects
- **Safe rollback for failed tool calls**
- Solves "no safe rollback" problem

#### **MT-AgentRisk** (arXiv:2602.13379)
- First benchmark for **multi-turn tool safety**
- **Attack Success Rate â†‘ 16%** from single-turn â†’ multi-turn
- **ToolShield**: Training-free defense
  - Self-exploration: agent generates tool test cases
  - Executes tests to observe effects
  - Distills safety experiences
  - **Reduces ASR by 30%** average

### ğŸ—ï¸ Supporting Infrastructure

- **vLLM**: Excellent throughput for production hosting
- **Text Generation Inference** (HuggingFace): Optimized deployment
- **Dataline**: LLM â†’ SQL for local database data sovereignty
- **Swirl Connect**: Simplifies RAG without extensive reformatting

---

## ğŸ› ï¸ Emerging Frameworks & Tools (Feb 2026)

### 1. **Agno** â€” Agentic Programming Language ğŸŒŸ

**ğŸŒŸ Concept**: Not just another framework â€” a **complete programming language** for agentic systems.

**Core Primitives**:
- Agents, teams, workflows
- Memory, knowledge, tools, guardrails
- Approval flows, governance

**Production Features**:
- 50+ built-in APIs
- Per-user session isolation
- Stateless, horizontally scalable runtime
- Runtime approval enforcement
- Background execution & scheduler
- Full auditability & observability

**ğŸš€ Example**: *Gcode* â€” coding agent that writes, reviews, iterates, remembers conventions, learns from mistakes.

**GitHub**: `github.com/agno-agi/agno`

---

### 2. **GitHub Copilot SDK v0.1.23** (Feb 6, 2026)

- Multi-platform SDK for programmatic Copilot Agent integration
- Production-tested agent runtime
- Programmatic agent control
- **Stars**: 7.1k+
- **Use Case**: Embed agentic coding into custom tools, platforms, CI/CD pipelines

---

### 3. **Kimi K2.5** â€” Multimodal Agent Swarm

**ğŸ”¥ Features** (as of Feb 2, 2026):
- Full video processing + native multimodal
- **Agent Swarm**: up to 100 concurrent sub-agents
- **1,500 tool calls** capacity per session
- **Kimi Code**: Open-source coding agent (VSCode, Cursor, Zed plugins)

**Status**: Repo exists, weights not yet public?

---

## ğŸ“Š Focus Area Summary (Feb 19, 2026)

| Area | Status | Hot Topics | Notable Releases |
|------|--------|------------|------------------|
| **Multimodal Agents** | Evolving | Agent Swarms, video understanding, GUI agents | Kimi K2.5, Agno, VILLAIN |
| **Mobile Deployment** | Mature â†’ Production | Quantization, edge offloading, world models | ECLD framework, Sarvam edge OS |
| **Tool Use** | Mature â†’ Secure | Transactional semantics, multi-turn safety | Atomix, ToolShield |
| **Inference Optimization** | Paradigm shift | Latent CoT, constraint training, topological analysis | vLLM GB200, CRT |

---

## ğŸ† Standout Discoveries

### ğŸ¯ Most Impactful Concept
**Transactional Semantics for Agents** (Atomix) â€” finally solves safe rollback, critical for production agents handling financial, medical, or legal operations.

### ğŸ“± Most Actionable Tech
**MobileLLM-R1.5** (1.7B CPU-optimized) + **ExecuTorch** deployment â€” enables on-device agents on existing phones without cloud dependency.

### ğŸ§  Most Philosophical Shift
**Latent CoT** â€” decouples reasoning from output, challenges the transparency-reasoning tradeoff, enabling smaller models to appear smarter.

### ğŸ› ï¸ Most Developer-Friendly
**Agno** â€” full programming language + runtime, not just another library; represents a paradigm shift in agent development.

---

## ğŸ“ˆ Signals to Watch (Next 24h)

### ğŸ”´ High Probability
- GLM-5 integration tutorials (Atlas Cloud, NVIDIA NIM)
- Atomix/ECLD GitHub repos star growth >1k/h
- ToolShield benchmark suite code release
- Community benchmarks for mobile models

### ğŸŸ¡ Medium Probability
- New arXiv papers citing today's research (Atomix, MT-AgentRisk, CRT)
- Agno adoption announcements from early users
- Mobile deployment deep-dive tutorials (ExecuTorch + MobileLLM)

### ğŸŸ¢ Low Probability
- Major new model launch (Chinese New Year lull)
- Breakthrough in agent self-improvement without oversight
- Regulatory announcements affecting agent deployments

---

## ğŸ’¡ Actionable Recommendations

### For ML Engineers
1. **Experiment with 4-bit QAT** on mobile models (<3GB memory footprint)
2. **Study PagedAttention internals** â€” vLLM's core optimization for production serving
3. **Try Latent CoT** on existing models for token and cost reduction

### For Agent Developers
1. **Implement multi-turn safety testing** (MT-AgentRisk methodology) before production deployment
2. **Build tool self-testing frameworks** (ToolShield approach) for robust agents
3. **Adopt transactional semantics** for critical tool calls (Atomix patterns), especially with external APIs

### For Researchers
1. **Explore reasoning topology** with TDA (topological data analysis) to identify redundant reasoning patterns
2. **Benchmark replication**: Test CRT on your models, compare with standard CoT
3. **Mobile edge optimization** â€” 12-30% performance gains still available through architectural innovations

---

## ğŸ“š Reference Resources

### Recent Papers (Discovered Today)
- [Atomix: Timely, Transactional Tool Use](https://arxiv.org/abs/2602.14849)
- [MT-AgentRisk: Unsafer in Many Turns](https://arxiv.org/abs/2602.13379) | [ToolShield Code](https://github.com/CHATS-Lab/ToolShield)
- **[ECLD Framework](https://arxiv.org/abs/2602.13628)** â€” Edge Compact LLM Deployment (Mobile Edge Computing)
  - 70-80% storage compression, up to 50% energy reduction
  - World model-PPO for smart load offloading (12-30% latency â†“)
- [CRT: Constraint-Rectified Training](https://arxiv.org/abs/2602.12526)
- [A2MAML: Multi-Agent Multimodal Learning](https://arxiv.org/abs/2602.04763)
- [VILLAIN: Multimodal Fact-Checking](https://arxiv.org/abs/2602.04587)

### Tools & Frameworks
- [Agno](https://github.com/agno-agi/agno) â€” Agentic programming language
- [GitHub Copilot SDK](https://github.com/github/copilot-sdk) â€” v0.1.23 (Feb 6, 2026)
- [ExecuTorch](https://pytorch.org/executorch/) â€” Mobile PyTorch deployment
- [ToolShield](https://github.com/CHATS-Lab/ToolShield) â€” Multi-turn safety defense

### Models
- [microGPT](https://gist.github.com/karpathy) â€” 200-line pure Python GPT
- [nanochat](https://github.com/karpathy/nanochat) â€” "$100 ChatGPT" with speedrun leaderboard
- [GLM-5](https://build.nvidia.com/z-ai/glm5/modelcard) â€” 744B MoE model from ZHIPU AI
- [MobileLLM-R1.5](https://ai.meta.com/) â€” CPU-optimized for mobile
- [Qwen3-0.6B](https://huggingface.co/Qwen) â€” Ultra-fast 0.6B parameter model

---

## ğŸ”® Next 12 Hours Forecast

**Expected**:
- GLM-5 integration demos and tutorials from early adopters
- Atomix repository public release (if not already) and community uptake
- ToolShield code release + benchmark results published
- Community comparisons between Latent CoT vs standard CoT on diverse tasks

**Potential**:
- New arXiv submissions citing these papers (Atomix, MT-AgentRisk, CRT, ECLD)
- Early adopter case studies (Agno in production, mobile agents in emerging markets)
- ExecuTorch + MobileLLM tutorials achieving sub-500ms latency on flagship phones

**Unlikely but Impactful**:
- Major new model launch (Chinese New Year period typically quiet)
- Breakthrough in agent self-improvement without human oversight
- Regulatory framework announcement specifically targeting AI agents

---

## ğŸ¯ Bottom Line

**The field is converging on reliability, safety, and efficiency**:

- **Reliability** â†’ **Transactional semantics** (Atomix) for safe tool execution with rollback
- **Safety** â†’ **Multi-turn defense** (ToolShield) against escalating attacks
- **Efficiency** â†’ **Mobile optimization** + **Latent CoT** for cost reduction
- **Developer Experience** â†’ **Agno** (language-level agent programming)

**No single breakthrough dominates** â€” it's a **wave of maturation** across the entire stack, moving agents from research labs into production environments with proper engineering practices.

**The most exciting trend**: Barriers to entry are dropping dramatically â€” from Karpathy's $100 GPT-2 training to mobile LLMs on existing phones, to Agno's programming language approach. 2026 is the year **democratized agent engineering** becomes reality.

---

*Scan completed: 2026-02-19 07:15 GMT+8*  
*Status: Independent subagent run; complements existing hourly scans.*  
*Next steps: Monitor GLM-5 adoption, ToolShield releases, and mobile deployment benchmarks.*
