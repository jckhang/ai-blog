---
id: 20260219-rss-lilian_weng-XXX-diffusion-models-for-video-generation
title: Diffusion Models for Video Generation
created: 2026-02-19
tags: ["rss", "engineering", "auto-import"]
source: "Lilian Weng's Blog"
source_url: "https://lilianweng.github.io/posts/2024-04-12-diffusion-video/"
source_type: "article"
content_length: 946
quality_score: 0.60
---

# Diffusion Models for Video Generation

## åŸæ–‡æ¦‚è§ˆ

- **æ¥æº**: Lilian Weng's Blog
- **å‘å¸ƒæ—¶é—´**: Fri, 12 Apr 2024 00:00:00 +0000
- **åŸæ–‡é“¾æ¥**: https://lilianweng.github.io/posts/2024-04-12-diffusion-video/
- **æŠ“å–æ—¶é—´**: 2026-02-19T20:08:30.099Z

## æ ¸å¿ƒå†…å®¹

[Diffusion models](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task&mdash;using it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:

<ol>
- It has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.

- In comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.

</ol>
<blockquote>

<b>
ğŸ¥‘ Required Pre-read: Please make sure you have read the previous blog on [&ldquo;What are Diffusion Models?&rdquo;](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) for image generation before continue here.
</b>

## å…³é”®è§‚ç‚¹

<!-- TODO: äººå·¥æˆ–LLMæå–3-5ä¸ªå…³é”®è§‚ç‚¹ -->

## ç›¸å…³é“¾æ¥

- [[TODO-æ·»åŠ ç›¸å…³ZKå¡ç‰‡]]

---
*RSS è‡ªåŠ¨æŠ“å– - æŠ“å–æ—¶é—´: 2026-02-19T20:08:30.099Z*