---
id: 20260220-rss-huggingface_blog-001-transformers-js-v4-preview-now-available-on-npm
title: Transformers.js v4 Preview: Now Available on NPM!
created: 2026-02-21
tags: ["rss","ai_research","auto-import","permanent"]
source: "Hugging Face Blog"
source_url: "https://huggingface.co/blog/transformersjs-v4"
source_type: "article"
content_length: 14419
quality_score: 0.95
---

# Transformers.js v4 Preview: Now Available on NPM!

## æ¥æºä¿¡æ¯

- **æ¥æº**: Hugging Face Blog
- **å‘å¸ƒæ—¶é—´**: è§åŸæ–‡
- **åŸæ–‡é“¾æ¥**: https://huggingface.co/blog/transformersjs-v4
- **é‡‡é›†æ—¶é—´**: 2026-02-21

## æ ¸å¿ƒå†…å®¹

const guestTheme = document.cookie.match(/theme=(\w+)/)?.[1]; document.documentElement.classList.toggle('dark', guestTheme === 'dark' || ( (!guestTheme || guestTheme === 'system') && window.matchMedia('(prefers-color-scheme: dark)').matches)); Transformers.js v4 Preview: Now Available on NPM! ((window.plausible = window.plausible || function () { (plausible.q = plausible.q || []).push(arguments); }), (plausible.init = plausible.init || function (i) { plausible.o = i || {}; })); plausible.init({ customProperties: { loggedIn: "false", }, endpoint: "/api/event", }); window.hubConfig = {"features":{"signupDisabled":false},"sshGitUrl":"git@hf.co","moonHttpUrl":"https:\/\/huggingface.co","captchaApiKey":"bd5f2066-93dc-4bdd-a64b-a24646ca3859","datasetViewerPublicUrl":"https:\/\/datasets-server.huggingface.co","stripePublicKey":"pk_live_x2tdjFXBCvXo2FFmMybezpeM00J6gPCAAc","environment":"production","userAgent":"HuggingFace (production)","spacesIframeDomain":"hf.space","spacesApiUrl":"https:\/\/api.hf.space","docSearchKey":"ece5e02e57300e17d152c08056145326e90c4bff3dd07d7d1ae40cf1c8d39cb6","logoDev":{"apiUrl":"https:\/\/img.logo.dev\/","apiKey":"pk_UHS2HZOeRnaSOdDp7jbd5w"}}; window.requestId = "Root=1-6998536a-17ba4b6f67cba2425b16a4d2"; window.featureFlags = {"buckets":false}; Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Back to Articles Transformers.js v4 Preview: Now Available on NPM! Published February 9, 2026 Update on GitHub Upvote 70 +64 Joshua Xenova Follow Nico Martin nico-martin Follow Performance & Runtime Improvements Repository Restructuring PNPM Workspaces Modular Class Structure Examples Repository Prettier New Models and Architectures New Build System Standalone Tokenizers.js Library Miscellaneous Improvements Acknowledgements We're excited to announce that Transformers.js v4 (preview) is now available on NPM! After nearly a year of development (we started in March 2025 ğŸ¤¯), we're finally ready for you to test it out. Previously, users had to install v4 directly from source via GitHub, but now it's as simple as running a single command! npm i @huggingface/transformers@next We'll continue publishing v4 releases under the next tag on NPM until the full release, so expect regular updates! Performance & Runtime Improvements The biggest change is undoubtedly the adoption of a new WebGPU Runtime, completely rewritten in C++. We've worked closely with the ONNX Runtime team to thoroughly test this runtime across our ~200 supported model architectures, as well as many new v4-exclusive architectures. In addition to better operator support (for performance, accuracy, and coverage), this new WebGPU runtime allows the same transformers.js code to be used across a wide variety of JavaScript environments, including browsers, server-side runtimes, and desktop applications. That's right, you can now run WebGPU-accelerated models directly in Node, Bun, and Deno! We've proven that it's possible to run state-of-the-art AI models 100% locally in the browser, and now we're focused on performance: making these models run as fast as possible, even in resource-constrained environments. This required completely rethinking our export strategy, especially for large language models. We achieve this by re-implementing new models operation by operation, leveraging specialized ONNX Runtime Contrib Operators like com.microsoft.GroupQueryAttention , com.microsoft.MatMulNBits , or com.microsoft.QMoE to maximize performance. For example, adopting the com.microsoft.MultiHeadAttention operator, we were able to achieve a ~4x speedup for BERT-based embedding models. This update enables full offline support by caching WASM files locally in the browser, allowing users to run Transformers.js applications without an internet connection after the initial download. Repository Restructuring Developing a new major version gave us the opportunity to invest in the codebase and tackle long-overdue refactoring efforts. PNPM Workspaces Until now, the GitHub repository served as our npm package. This worked well as long as the repository only exposed a single library. However, looking to the future, we saw the need for various sub-packages that depend heavily on the Transformers.js core while addressing different use cases, like library-specific implementations, or smaller utilities that most users don't need but are essential for some. That's why we converted the repository to a monorepo using pnpm workspaces. This allows us to ship smaller packages that depend on @huggingface/transformers without the overhead of maintaining separate repositories. Modular Class Structure Another major refactoring effort targeted the ever-growing models.js file. In v3, all available models were defined in a single file spanning over 8,000 lines, becoming increasingly difficult to maintain. For v4, we split this into smaller, focused modules with a clear distinction between utility functions, core logic, and model-specific implementations. This new structure improves readability and makes it much easier to add new models. Developers can now focus on model-specific logic without navigating through thousands of lines of unrelated code. Examples Repository In v3, many Transformers.js example projects lived directly in the main repository. For v4, we've moved them to a dedicated repository , allowing us to maintain a cleaner codebase focused on the core library. This also makes it easier for users to find and contribute to examples without sifting through the main repository. Prettier We updated the Prettier configuration and reformatted all files in the repository. This ensures consistent formatting throughout the codebase, with all future PRs automatically following the same style. No more debates about formatting... Prettier handles it all, keeping the code clean and readable for everyone. New Models and Architectures Thanks to our new export strategy and ONNX Runtime's expanding support for custom operators, we've been able to add many new models and architectures to Transformers.js v4. These include popular models like GPT-OSS, Chatterbox, GraniteMoeHybrid, LFM2-MoE, HunYuanDenseV1, Apertus, Olmo3, FalconH1, and Youtu-LLM. Many of these required us to implement support for advanced architectural patterns, including Mamba (state-space models), Multi-head Latent Attention (MLA), and Mixture of Experts (MoE). Perhaps most importantly, these models are all compatible with WebGPU, allowing users to run them directly in the browser or server-side JavaScript environments with hardware acceleration. Stay tuned for some exciting demos showcasing these new models in action! New Build System We've migrated our build system from Webpack to esbuild, and the results have been incredible. Build times dropped from 2 seconds to just 200 milliseconds, a 10x improvement that makes development iteration significantly faster. Speed isn't the only benefit, though: bundle sizes also decreased by an average of 10% across all builds. The most notable improvement is in transformers.web.js, our default export, which is now 53% smaller, meaning faster downloads and quicker startup times for users. Standalone Tokenizers.js Library A frequent request from users was to extract the tokenization logic into a separate library, and with v4, that's exactly what we've done. @huggingface/tokenizers is a complete refactor of the tokenization logic, designed to work seamlessly across browsers and server-side runtimes. At just 8.8kB (gzipped) with zero dependencies, it's incredibly lightweight while remaining fully type-safe. See example code import { Tokenizer } from "@huggingface/tokenizers" ; // Load from Hugging Face Hub const modelId = "HuggingFaceTB/SmolLM3-3B" ; const tokenizerJson = await fetch ( `https://huggingface.co/ ${modelId} /resolve/main/tokenizer.json` ). then ( res => res. json ()); const tokenizerConfig = await fetch ( `https://huggingface.co/ ${modelId} /resolve/main/tokenizer_config.json` ). then ( res => res. json ()); // Create tokenizer const tokenizer = new Tokenizer (tokenizerJson, tokenizerConfig); // Tokenize text const tokens = tokenizer. tokenize ( "Hello World" ); // ['Hello', 'Ä World'] const encoded = tokenizer. encode ( "Hello World" ); // { ids: [9906, 4435], tokens: ['Hello', 'Ä World'], ... } This separation keeps the core of Transformers.js focused and lean while offering a versatile, standalone tool that any WebML project can use independently. Miscellaneous Improvements We've made several quality-of-life improvements across the library. The type system has been enhanced with dynamic pipeline types that adapt based on inputs, providing better developer experience and type safety. Logging has been improved to give users more control and clearer feedback during model execution. Additionally, we've added support for larger models exceeding 8B parameters. In our tests, we've been able to run GPT-OSS 20B (q4f16) at ~60 tokens per second on an M4 Pro Max. Acknowledgements We want to extend our heartfelt thanks to everyone who contributed to this major release, especially the ONNX Runtime team for their incredible work on the new WebGPU runtime and their support throughout development, as well as all external contributors and early testers. More Articles from our Blog announcement transformers.js transformers Transformers.js v3: WebGPU Support, New Models & Tasks, and Moreâ€¦ 82 October 22, 2024 transformers v5 community Hot Transformers v5: Simple model definitions powering the AI ecosystem 298 December 1, 2025 Community any examples yet in Spaces? \n&quot;,&quot;updatedAt&quot;:&quot;2026-02-10T21:09:21.016Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;67d34970b25bfa43c4f3badf&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VYiYbY3pCJREWqJG6ls75.png&quot;,&quot;fullname&quot;:&quot;Juan Pinzon&quot;,&quot;name&quot;:&quot;juanpin&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;isUserFollowing&quot;:false}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.7624499201774597},&quot;editors&quot;:[&quot;juanpin&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VYiYbY3pCJREWqJG6ls75.png&quot;],&quot;reactions&quot;:[{&quot;reaction&quot;:&quot;ğŸš€&quot;,&quot;users&quot;:[&quot;AkujinLiffy&quot;],&quot;count&quot;:1}],&quot;isReport&quot;:false},&quot;replies&quot;:[{&quot;id&quot;:&quot;698f609c450ff984b8a5d912&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;656234362324e4d25cbde1ff&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/656234362324e4d25cbde1ff/RYWxAXiDlQ18dpwmR5kmc.png&quot;,&quot;fullname&quot;:&quot;Paul S&quot;,&quot;name&quot;:&quot;SuperPauly&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:4,&quot;isUserFollowing&quot;:false},&quot;createdAt&quot;:&quot;2026-02-13T17:34:20.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;Yeah bro, here https://huggingface.co/collections/webml-community/transformersjs-v4-demos&quot;,&quot;html&quot;:&quot; Yeah bro, here https://huggingface.co/collections/webml-community/transformersjs-v4-demos \n&quot;,&quot;updatedAt&quot;:&quot;2026-02-13T17:34:20.234Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;656234362324e4d25cbde1ff&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/656234362324e4d25cbde1ff/RYWxAXiDlQ18dpwmR5kmc.png&quot;,&quot;fullname&quot;:&quot;Paul S&quot;,&quot;name&quot;:&quot;SuperPauly&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:4,&quot;isUserFollowing&quot;:false}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.5711881518363953},&quot;editors&quot;:[&quot;SuperPauly&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/656234362324e4d25cbde1ff/RYWxAXiDlQ18dpwmR5kmc.png&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false,&quot;parentCommentId&quot;:&quot;698b9e8117e4463f5b925f5c&quot;}}]},{&quot;id&quot;:&quot;69974dcc98757c71d53d4002&quot;,&quot;createdAt&quot;:&quot;2026-02-19T17:52:12.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:true,&quot;hidden&quot;:true,&quot;hiddenBy&quot;:&quot;&quot;,&quot;latest&quot;:{&quot;raw&quot;:&quot;This comment has been hidden&quot;,&quot;html&quot;:&quot;This comment has been hidden&quot;,&quot;updatedAt&quot;:&quot;2026-02-20T08:32:13.966Z&quot;},&quot;numEdits&quot;:1,&quot;editors&quot;:[],&quot;editorAvatarUrls&quot;:[],&quot;reactions&quot;:[]}}],&quot;status&quot;:&quot;open&quot;,&quot;isReport&quot;:false,&quot;pinned&quot;:false,&quot;locked&quot;:false,&quot;collection&quot;:&quot;community_blogs&quot;},&quot;contextAuthors&quot;:[&quot;Xenova&quot;,&quot;nico-martin&quot;],&quot;primaryEmailConfirmed&quot;:false,&quot;discussionRole&quot;:0,&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;withThread&quot;:true,&quot;cardDisplay&quot;:false,&quot;repoDiscussionsLocked&quot;:false}"> juanpin 10 days ago any examples yet in Spaces? 1 reply Â· ğŸš€ 1 1 + SuperPauly 7 days ago Yeah bro, here https://huggingface.co/collections/webml-community/transformersjs-v4-demos deleted about 19 hours ago This comment has been hidden Edit Preview Upload images, audio, and videos by dragging in the text input, pasting, or clicking here . Tap or paste here to upload images Comment Â· Sign up or log in to comment Upvote 70 +58 System theme Company TOS Privacy About Careers Website Models Datasets Spaces Pricing Docs import("\/front\/build\/kube-8f23bef\/index.js"); window.moonSha = "kube-8f23bef\/"; window.__hf_deferred = {}; if (["hf.co", "huggingface.co"].includes(window.location.hostname)) { const script = document.createElement("script"); script.src = "https://js.stripe.com/v3/"; script.async = true; document.head.appendChild(script); }

## å…³é”®è§‚ç‚¹

- æ ¸å¿ƒä¸»é¢˜æ¶‰åŠTransformers.jså’ŒPreview:
- ç ”ç©¶æ¥æº: Hugging Face Blog
- å†…å®¹è´¨é‡è¯„åˆ†: 0.95

## æ·±åº¦æ€è€ƒ

- è¿™é¡¹ç ”ç©¶çš„æ ¸å¿ƒå‡è®¾æ˜¯ä»€ä¹ˆï¼Ÿå®ƒä»¬æ˜¯å¦åˆç†ï¼Ÿ
- æ–¹æ³•çš„ä¸»è¦å±€é™æ€§æ˜¯ä»€ä¹ˆï¼Ÿæœªæ¥å¦‚ä½•æ”¹è¿›ï¼Ÿ
- è¿™ä¸ªå‘ç°å¯¹å®é™…åº”ç”¨åœºæ™¯æœ‰ä»€ä¹ˆå¯ç¤ºï¼Ÿ
- å¦‚æœæ¢ä¸€ä¸ªæ•°æ®é›†æˆ–ç¯å¢ƒï¼Œç»“æœæ˜¯å¦ä¾ç„¶æˆç«‹ï¼Ÿ
- è¿™é¡¹å·¥ä½œçš„åˆ›æ–°ç‚¹æ˜¯å¦è¢«å……åˆ†è®ºè¯ï¼Ÿ

## ç›¸å…³é“¾æ¥

- [[001-zettelkasten-æ˜¯ä»€ä¹ˆ]]
- [[009-å†™ä½œå°±æ˜¯æ€è€ƒ]]
- [[011-ä»ç¬”è®°åˆ°æ–‡ç« ]]


*RSS è‡ªåŠ¨é‡‡é›†æ°¸ä¹…åŒ– - å¤„ç†æ—¶é—´: 2026-02-21*
