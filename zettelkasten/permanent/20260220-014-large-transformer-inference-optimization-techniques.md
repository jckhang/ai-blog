---
id: 20260220-014-large-transformer-inference-optimization-techniques
title: Large Transformeræ¨ç†ä¼˜åŒ–æŠ€æœ¯å…¨æ™¯
created: 2026-02-20
tags: ["transformer", "inference-optimization", "quantization", "pruning", "sparsity", "moe", "distillation", "mobile-ai"]
source: "Lilian Weng Blog - Large Transformer Model Inference Optimization (2023-01-10)"
source_url: "https://lilianweng.github.io/posts/2023-01-10-inference-optimization/"
content_length: 4200
quality_score: 0.94
related_notes: ["20260220-011-edgeNav-qe-mobile-ai-optimization", "20260220-009-step3-vl-10b-architecture-deep-dive", "20260220-002-llm-powered-autonomous-agents"]
---

# Large Transformeræ¨ç†ä¼˜åŒ–æŠ€æœ¯å…¨æ™¯

> **æ ¸å¿ƒé—®é¢˜**: Transformeræ¨ç†åœ¨æ—¶é—´å’Œå†…å­˜ä¸Šæˆæœ¬æé«˜ï¼Œé˜»ç¢å¤§è§„æ¨¡åº”ç”¨
> **ä¸¤å¤§ç“¶é¢ˆ**: (1) KV cacheå†…å­˜å ç”¨å·¨å¤§ (2) Attentionè®¡ç®—å¤æ‚åº¦O(nÂ²)
> **è§£å†³æ–¹æ¡ˆ**: é‡åŒ–ã€å‰ªæã€ç¨€ç–åŒ–ã€è’¸é¦ã€æ¶æ„ä¼˜åŒ–ï¼ˆ5å¤§ç±»ï¼‰
> **ç›®æ ‡**: æ‰‹æœºç«¯éƒ¨ç½²ï¼ˆ<3GBå†…å­˜ï¼Œ<500mså»¶è¿Ÿï¼‰

---

## ğŸ¯ ä¸ºä»€ä¹ˆTransformeræ¨ç†å¦‚æ­¤æ˜‚è´µï¼Ÿ

### æŒ‘æˆ˜1: å·¨å¤§å†…å­˜å ç”¨

**KV Cacheé—®é¢˜**:
- å¯¹äºbatch size=512, context length=2048, æ€»KV cache = **3TB** (!)
- ç›¸å½“äºæ¨¡å‹å‚æ•°çš„3å€

**æ¿€æ´»å€¼**:
- ä¸­é—´å±‚è¾“å‡ºï¼ˆactivationï¼‰éœ€è¦ä¿ç•™ç”¨äºbackwardï¼ˆè®­ç»ƒï¼‰æˆ–æ¨ç†ä¸­çš„æ³¨æ„åŠ›è®¡ç®—
- å¯¹äº10Bæ¨¡å‹ï¼Œæ¿€æ´»å€¼å¯è¾¾10-20GB

---

### æŒ‘æˆ˜2: äºŒæ¬¡æ–¹å¤æ‚åº¦

```
Attentionè®¡ç®—: O(nÂ² Ã— d) å…¶ä¸­n=sequence length, d=hidden dim
å½“n=2048, d=8192 â†’ è®¡ç®—é‡å·¨å¤§
```

**å½±å“**:
- é•¿æ–‡æœ¬ç”Ÿæˆï¼ˆç¼–ç¨‹ã€å†™ä½œï¼‰é€Ÿåº¦æ…¢
- æ— æ³•åœ¨èµ„æºå—é™è®¾å¤‡è¿è¡Œ

---

### æŒ‘æˆ˜3: å¹¶è¡Œæ€§å·®

**è‡ªå›å½’ç”Ÿæˆ** (autoregressive):
- æ¯ä¸ªtokenå¿…é¡»ç­‰å¾…å‰ä¸€ä¸ªç”Ÿæˆå®Œæˆ
- æ— æ³•åƒè®­ç»ƒé‚£æ ·å¹¶è¡Œæ‰€æœ‰ä½ç½®

**å¯¹æ¯”**:
- è®­ç»ƒ: å¯å¹¶è¡Œæ•´ä¸ªåºåˆ— â†’ é«˜åå
- æ¨ç†: é€æ­¥ç”Ÿæˆ â†’ ä½ååã€é«˜å»¶è¿Ÿ

---

## ğŸ”§ ä¼˜åŒ–æ–¹æ³•åˆ†ç±»ï¼ˆ5å¤§ç±»ï¼‰

æ ¹æ®Lilian Wengçš„ç»¼è¿°ï¼Œä¼˜åŒ–åˆ†ä¸ºï¼š

| ç±»åˆ« | æ ¸å¿ƒæ€æƒ³ | æ•ˆæœ | ä»£ä»· |
|------|---------|------|------|
| **1. å¹¶è¡ŒåŒ–** | æ•°æ®/æ¨¡å‹å¹¶è¡Œ | ååâ†‘ï¼Œå»¶è¿Ÿä¸å˜ | éœ€å¤šGPU |
| **2. å†…å­˜å¸è½½** | CPU/GPUå†…å­˜äº¤æ¢ | å†…å­˜â†“ï¼Œå»¶è¿Ÿâ†‘â†‘ | æ…¢ |
| **3. æ™ºèƒ½æ‰¹å¤„ç†** | åŠ¨æ€pack padding | ååâ†‘ | éœ€è°ƒä¼˜ |
| **4. ç½‘ç»œå‹ç¼©** | é‡åŒ–/å‰ªæ/è’¸é¦ | å†…å­˜â†“â†“ï¼Œé€Ÿåº¦â†‘ï¼Œç²¾åº¦â†“ | éœ€è®­ç»ƒ/å¾®è°ƒ |
| **5. æ¶æ„ä¼˜åŒ–** | ä¿®æ”¹æ³¨æ„åŠ›ã€FFN | å†…å­˜â†“ï¼Œé€Ÿåº¦â†‘ | éœ€é‡æ–°è®­ç»ƒ |

**é‡ç‚¹**: æ‰‹æœºç«¯é€‚ç”¨ **4&5ç±»**ï¼ˆå‹ç¼© + æ¶æ„ä¼˜åŒ–ï¼‰

---

## ğŸ“‰ Network Compression Techniquesï¼ˆæŠ€æœ¯ç»†èŠ‚ï¼‰

### 1. çŸ¥è¯†è’¸é¦ (Knowledge Distillation)

**ç†å¿µ**: ç”¨å¤§æ¨¡å‹ï¼ˆTeacherï¼‰æ•™å°æ¨¡å‹ï¼ˆStudentï¼‰

**æŸå¤±å‡½æ•°**:
```
L_KD = L_distill(softmax(z_t, T), softmax(z_s, T)) + Î» L_CE(y, z_s)
```
- `T`: é«˜æ¸©ï¼ˆsoftenæ¦‚ç‡åˆ†å¸ƒï¼‰
- `Î»`: å¹³è¡¡è½¯æ ‡ç­¾å’Œç¡¬æ ‡ç­¾

**æˆåŠŸæ¡ˆä¾‹**: DistilBERT
- å‚æ•°é‡å‡å°‘40%
- ä¿æŒ97%æ€§èƒ½
- æ¨ç†é€Ÿåº¦æå‡71%

**å¯¹äºæ‰‹æœºAgent**:
- Teacher: STEP3-VL-10B (æˆ–æ›´å¤§)
- Student: 3B-4Bæ¨¡å‹
- æ•°æ®: æ‰‹æœºæ“ä½œè½¨è¿¹ + é€šç”¨å¯¹è¯
- ç›®æ ‡: ä¿æŒ90%æ€§èƒ½ï¼Œå†…å­˜é™è‡³3GB

---

### 2. é‡åŒ– (Quantization)

#### 2.1 Post-Training Quantization (PTQ)

**ä¼˜åŠ¿**: æ— éœ€è®­ç»ƒï¼Œå¿«é€Ÿéƒ¨ç½²  
**åŠ£åŠ¿**: ç²¾åº¦æŸå¤±è¾ƒå¤§

**æ··åˆç²¾åº¦ç­–ç•¥**:
- **W8A32**: æƒé‡8-bit + æ¿€æ´»32-bit â†’ æ•ˆæœå¥½ä½†å†…å­˜çœæœ‰é™
- **W8A8**: å…¨8-bit â†’ çœå†…å­˜ï¼Œä½†æ¿€æ´»å¼‚å¸¸å€¼å¯¼è‡´æ€§èƒ½ä¸‹é™

**å…³é”®å‘ç°** (Bondarenko et al. 2021):
> FFNå±‚çš„è¾“å‡ºæœ‰æç«¯outliersï¼Œå¯¼è‡´per-tensoré‡åŒ–å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**: å¯¹å¼‚å¸¸æ¿€æ´»ä¿ç•™FP16ï¼Œå…¶ä»–é‡åŒ–åˆ°INT8

---

#### 2.2 LLM.int8() (Dettmers et al. 2022)

**æ ¸å¿ƒåˆ›æ–°**: æ··åˆç²¾åº¦åˆ†è§£

1. **è¡Œçº§é‡åŒ–**: æ¯ä¸ªinner productç‹¬ç«‹ç¼©æ”¾
2. **å¼‚å¸¸å€¼åˆ†ç¦»**: Outlier featuresä¿æŒFP16ï¼ˆä»…å <1%æƒé‡ï¼‰

**å…¬å¼**:
```
å¯¹äºæ¿€æ´»X:
  - æ¯è¡Œæ‰¾abs_max
  - é‡åŒ–åˆ°INT8
  - å¦‚æœæŸç»´åº¦>20Ã—medianï¼Œä¿ç•™FP16

å¯¹äºæƒé‡W:
  - åŒæ ·per-rowé‡åŒ–
  - Outlierç»´åº¦æ ‡è®°
```

**æ•ˆæœ**: 175B OPTæ¨¡å‹é‡åŒ–åˆ°8-bitï¼Œæ€§èƒ½æŸå¤±<1%

**å¯¹äºæ‰‹æœº**: 
- 10Bæ¨¡å‹ â†’ INT8å1.25GBï¼Œä½†å†…å­˜å ç”¨ä»é«˜
- éœ€è¦ç»“åˆ4-bité‡åŒ–

---

#### 2.3 SmoothQuant (Xiao & Lin 2022)

**é—®é¢˜**: æ¿€æ´»æ›´éš¾é‡åŒ–ï¼ˆoutliersåœ¨æ¿€æ´»ï¼Œä¸åœ¨æƒé‡ï¼‰

**åˆ›æ–°**: å°†outliersä»æ¿€æ´»è¿ç§»åˆ°æƒé‡ï¼ˆæ•°å­¦ç­‰ä»·å˜æ¢ï¼‰

**å…¬å¼**:
```
Y = (X * diag(s)^(-1)) Â· (diag(s) * W) = XÌ‚ Â· Å´
```
- `s`: smoothing factor, æ§åˆ¶è¿ç§»ç¨‹åº¦
- `Î±=0.5`: å¹³è¡¡æ¿€æ´»å’Œæƒé‡é‡åŒ–éš¾åº¦

**æ•ˆæœ**: å®ç°W8A8ï¼Œç¡¬ä»¶æ•ˆç‡é«˜äºæ··åˆç²¾åº¦

---

#### 2.4 GPTQ (Frantar et al. 2022)

**ç›®æ ‡**: æƒé‡é‡åŒ–åˆ°3-4 bitï¼Œå‡ ä¹æ— æŸ

**æ–¹æ³•**: é€è¡Œè´ªå©ªé‡åŒ–
1. å°†æƒé‡çŸ©é˜µWçœ‹ä½œè¡Œå‘é‡é›†åˆ {w_i}
2. å¯¹æ¯è¡Œï¼Œå¯»æ‰¾æœ€ä¼˜INT4è¡¨ç¤º
3. åˆ©ç”¨HessiançŸ©é˜µè¯„ä¼°é‡åŒ–è¯¯å·®

**ç»“æœ**: OPT-175Bé‡åŒ–åˆ°4-bitï¼Œç²¾åº¦æŸå¤±<1%

**å±€é™**: ä»…é€‚ç”¨äºæƒé‡ï¼Œä¸å¤„ç†æ¿€æ´»

---

### 3. å‰ªæ (Pruning)

#### 3.1 ç»“æ„åŒ– vs éç»“æ„åŒ–

- **éç»“æ„åŒ–**: ä»»æ„åˆ é™¤æƒé‡ â†’ ç¡¬ä»¶ä¸å‹å¥½ï¼Œæ— åŠ é€Ÿ
- **ç»“æ„åŒ–**: åˆ é™¤æ•´ä¸ªé€šé“/å¤´ â†’ ä¿æŒçŸ©é˜µä¹˜æ³•å½¢å¼ï¼ŒçœŸåŠ é€Ÿ

**æ‰‹æœºç«¯åªè€ƒè™‘ç»“æ„åŒ–å‰ªæ**

---

#### 3.2 N:Mç»“æ„åŒ–ç¨€ç–

**å®šä¹‰**: æ¯Mä¸ªè¿ç»­å…ƒç´ ä¸­ï¼Œæœ‰Nä¸ªä¸º0

**ç¤ºä¾‹**: 2:4ç¨€ç–ï¼ˆA100 GPUæ”¯æŒï¼‰
- æ¯4ä¸ªå…ƒç´ ï¼Œ2ä¸ªä¸º0
- Tensor Coreä¸“ç”¨ç¡¬ä»¶åŠ é€Ÿ

**ä¼˜åŠ¿**:
- ä¿æŒç¡¬ä»¶å‹å¥½æ ¼å¼
- æ¨ç†é€Ÿåº¦æå‡1.5-2Ã—

**å®ç°æµç¨‹** (Nvidiaå»ºè®®):
1. è®­ç»ƒdenseæ¨¡å‹è‡³æ”¶æ•›
2. å‰ªæè‡³ç›®æ ‡N:Mæ¨¡å¼ï¼ˆå¦‚2:4ï¼‰
3. Retrainæ¢å¤æ€§èƒ½

---

#### 3.3 é€šé“æ’åˆ—ä¼˜åŒ– (Channel Permutation)

**é—®é¢˜**: ç›´æ¥æŒ‰é»˜è®¤é€šé“å‰ªæï¼Œä¼šä¿ç•™ä½å¹…å€¼æƒé‡

**è§£å†³**: é‡æ–°æ’åˆ—é€šé“é¡ºåºï¼Œä½¿é«˜å¹…å€¼æƒé‡ç¬¦åˆN:Mæ¨¡å¼

**ç®—æ³•** (Pool & Yu 2021):
```
é‡å¤ä»¥ä¸‹ç›´åˆ°æ”¶æ•›:
  å¯¹æ¯å¯¹é€šé“ï¼Œå°è¯•äº¤æ¢
  å¦‚æœäº¤æ¢åæ€»å¹…å€¼å¢åŠ  â†’ æ¥å—
  å¦åˆ™ â†’ æ‹’ç»
```

**æ•ˆæœ**: ç›¸åŒç¨€ç–ç‡ä¸‹ï¼Œç²¾åº¦æå‡2-3pp

---

### 4. Mixture-of-Experts (MoE)

**ç†å¿µ**: æ¯ä¸ªtokenåªæ¿€æ´»éƒ¨åˆ†ä¸“å®¶ï¼ˆsparse activationï¼‰

**å…¬å¼**:
```
Expert capacity = round(C Ã— k Ã— batch_tokens / num_experts)
```
- `C`: capacity factor (é€šå¸¸1.0-1.5)
- `k`: æ¯ä¸ªtokenæ¿€æ´»çš„expertæ•°ï¼ˆé€šå¸¸2ï¼‰

**ä¼˜åŠ¿**:
- æ¿€æ´»å‚æ•°å°‘ â†’ æ¨ç†å¿«
- æ€»å‚æ•°é‡å¤§ â†’ å®¹é‡é«˜

**ç¤ºä¾‹**: Mixtral 8x7B
- æ€»å‚æ•°: 47B
- æ¿€æ´»å‚æ•°: ~13B per token
- æ¨ç†é€Ÿåº¦ â‰ˆ 7Bæ¨¡å‹ï¼Œæ€§èƒ½ â‰ˆ 13B

**æ‰‹æœºç«¯æŒ‘æˆ˜**:
- ä¸“å®¶è´Ÿè½½ä¸å‡è¡¡ï¼ˆæŸäº›expertè¢«é¢‘ç¹è®¿é—®ï¼‰
- éœ€è¦åŠ¨æ€è°ƒåº¦
- æš‚ä¸é€‚åˆèµ„æºæåº¦å—é™åœºæ™¯

---

### 5. æ¶æ„ä¼˜åŒ–ï¼ˆSparse Attentionï¼‰

**ç›®æ ‡**: é™ä½attentionè®¡ç®—å¤æ‚åº¦ O(nÂ² â†’ n)

#### 5.1 å›ºå®šæ¨¡å¼ (Fixed Patterns)

- **Blockwise**: å°†åºåˆ—åˆ†å—ï¼Œå—å†…è®¡ç®—attention
- **Local**: æ¯ä¸ªä½ç½®åªçœ‹å‰åkä¸ªtoken
- **Strided**: æ¯éš”kä¸ªä½ç½®å–key

**å…¬å¼**: å¤æ‚åº¦ O(bÃ—n) æˆ– O(sÃ—n)  
**ç¼ºç‚¹**: æ— æ³•å»ºæ¨¡é•¿è·ç¦»ä¾èµ–

---

#### 5.2 å­¦ä¹ æ¨¡å¼ (Learnable Patterns)

- **Reformer (LSH)**: ç”¨å±€éƒ¨æ•æ„Ÿå“ˆå¸Œèšç±»token
- **Routing Transformer**: k-meansèšç±»åattention
- **Sinkhorn Sorting**: å­¦ä¹ optimal tokenæ’åº

**ä¼˜åŠ¿**: è‡ªé€‚åº”é€‰æ‹©attention pattern  
**ä»£ä»·**: è®­ç»ƒå¤æ‚åº¦å¢åŠ 

---

#### 5.3 å¾ªç¯æœºåˆ¶ (Recurrence)

- **Transformer-XL**: é‡ç”¨å‰ä¸€æ®µçš„hidden stateï¼ˆsegment recurrenceï¼‰
- **Universal Transformer**: å›ºå®šæ­¥æ•°å¾ªç¯å¤„ç†
- **Compressive Transformer**: å‹ç¼©è®°å¿†å­˜å‚¨

**æ•ˆæœ**: æœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•ï¼ˆä»2kâ†’32k+ï¼‰

---

#### 5.4 å†…å­˜èŠ‚çœè®¾è®¡ (Memory Saving)

- **Linformer**: Key/ValueæŠ•å½±åˆ°ä½ç»´ç©ºé—´ (nÃ—n â†’ nÃ—k)
- **Multi-Query Attention**: å¤šå¤´å…±äº«åŒä¸€å¥—KV â†’ å†…å­˜â†“75%
- **Random Feature Attention**: æ ¸æ–¹æ³•è¿‘ä¼¼attention

**å¯¹äºæ‰‹æœº**:
- **Multi-Query Attention** å·²é‡‡ç”¨ï¼ˆQwenç³»åˆ—ï¼‰
- **Linformer** åœ¨çŸ­åºåˆ—ä¸‹æ•ˆæœè‰¯å¥½

---

## ğŸ¯ å¯¹æ‰‹æœºAgentçš„å…·ä½“åº”ç”¨

ç»“åˆ **EdgeNav-QE** (ç¬”è®°011) å’Œæœ¬ç¬”è®°ï¼Œåˆ¶å®šå®Œæ•´æ–¹æ¡ˆï¼š

### **ä¼˜åŒ–ç»„åˆç­–ç•¥**

| æŠ€æœ¯ | åº”ç”¨åœºæ™¯ | é¢„æœŸæ”¶ç›Š | å®ç°éš¾åº¦ |
|------|---------|---------|---------|
| **4-bité‡åŒ–** | æ‰€æœ‰æƒé‡ | å†…å­˜â†“75% (20GBâ†’5GB) | ç®€å• (bitsandbytes) |
| **QLoRA** | å¾®è°ƒé˜¶æ®µ | ä¿æŒç²¾åº¦ï¼Œè®­ç»ƒæˆæœ¬â†“ | ä¸­ç­‰ (PEFT) |
| **Early Exit** | æ¨ç†é˜¶æ®µ | å»¶è¿Ÿâ†“40%ï¼Œç®€å•æ ·æœ¬ | ä¸­ç­‰ (åŠ exit head) |
| **Mixed Precision** | æ¿€æ´»å€¼ | å¹³è¡¡outliers | ç®€å• (LLM.int8) |
| **SmoothQuant** | W8A8åœºæ™¯ | å…¨é‡åŒ–åŠ é€Ÿ | å›°éš¾ (éœ€é‡è®­ç»ƒ) |
| **Grouped Query** | æ³¨æ„åŠ› | KV cacheâ†“75% | éœ€è¦æ¨¡å‹æ”¯æŒ |
| **N:M Sparsity** | æƒé‡ | ç¨€ç–åŠ é€Ÿ (A100) | æéš¾ (è®­ç»ƒå™¨æ”¯æŒ) |

**æ¨èç»„åˆ** (æ‰‹æœºç«¯):
```
Base Model: Step3-VL-10B æˆ– Qwen3-4B
  â†“
Mixed Precision (LLM.int8-style)
  â†“
4-bit Quantization (GPTQæˆ–AWQ)
  â†“
Early Exit Heads (å¯é€‰)
  â†“
Grouped Query Attention (å¦‚æœæ¨¡å‹æ”¯æŒ)
```

**ç›®æ ‡æŒ‡æ ‡**:
- å†…å­˜: 4-5 GB
- å»¶è¿Ÿ: 500ms (simple), 1.2s (complex)
- å‡†ç¡®ç‡: >90% (å¯¹æ¯”full-precision)

---

### **å®éªŒè®¾è®¡**ï¼ˆé’ˆå¯¹EdgeNavç»­ç¯‡ï¼‰

**Step 1: åŸºçº¿æµ‹é‡**
- Full precision (FP16) on server
- è®°å½•å‡†ç¡®ç‡ (ScreenSpot-V2), å»¶è¿Ÿ (A100), å†…å­˜

**Step 2: å•ç‹¬æŠ€æœ¯æµ‹è¯•**
- 4-bit only
- 8-bit only
- Early Exit only
- Grouped Query only

**Step 2.5: ç»„åˆæ•ˆæœ**
- 4-bit + Early Exit
- 4-bit + GQA
- 8-bit + Early Exit

**Step 3: æœ€ä¼˜ç»„åˆ**
- åœ¨æ‰‹æœºç«¯éƒ¨ç½²æœ€ä½³ç»„åˆ
- æµ‹é‡å®é™…å»¶è¿Ÿï¼ˆè®¾å¤‡ï¼šSnapdragon 8 Gen 2ï¼‰
- åŠŸè€—æµ‹è¯•ï¼ˆmAh/100 inferencesï¼‰

**é¢„æœŸæœ€ä½³**: 4-bit + Early Exit + GQA â‰ˆ 3.5GB, 450ms, 91% acc

---

## ğŸ“Š æŠ€æœ¯é€‰å‹å†³ç­–çŸ©é˜µ

é’ˆå¯¹æ™ºè·ƒåƒé‡Œçš„æ‰‹æœºAgenté¡¹ç›®ï¼š

| æŠ€æœ¯ | å¯è¡Œæ€§ | æ•ˆæœ | ä¼˜å…ˆçº§ | å†³ç­– |
|------|-------|------|-------|------|
| **é‡åŒ– (4-bit)** | é«˜ (bitsandbytesæˆç†Ÿ) | â˜…â˜…â˜…â˜…â˜… | P0 | âœ… å¿…é€‰ |
| **Early Exit** | é«˜ (åŠ headå³å¯) | â˜…â˜…â˜…â˜…â˜† | P0 | âœ… å¿…é€‰ |
| **Grouped Query** | ä¸­ (éœ€æ¨¡å‹æ”¯æŒ) | â˜…â˜…â˜…â˜…â˜† | P1 | âš ï¸ é€‰æ¨¡å‹æ—¶è€ƒè™‘ |
| **SmoothQuant** | ä½ (éœ€é‡è®­ç»ƒ) | â˜…â˜…â˜…â˜†â˜† | P2 | â³ æœªæ¥è€ƒè™‘ |
| **MoE** | ä½ (æ¨ç†è°ƒåº¦å¤æ‚) | â˜…â˜…â˜…â˜…â˜… | P2 | â³ äº‘ç«¯ç”¨ |
| **N:M Sparsity** | æä½ (ç¡¬ä»¶ä¾èµ–) | â˜…â˜…â˜…â˜…â˜† | P3 | âŒ æ‰‹æœºä¸è€ƒè™‘ |

---

## ğŸ”¬ å…³é”®è®ºæ–‡ä¸å®ç°

| æŠ€æœ¯ | è®ºæ–‡ | å®ç° |
|------|------|------|
| **LLM.int8()** | Dettmers et al. 2022 | `bitsandbytes` |
| **SmoothQuant** | Xiao & Lin 2022 | `SmoothQuant` GitHub |
| **GPTQ** | Frantar et al. 2022 | `auto-gptq` |
| **AWQ** | Lin et al. 2023 | `AutoAWQ` |
| **Grouped Query** | Shazeer 2019 | `transformers`å†…ç½® |
| **Early Exit** | CALMç­‰ | è‡ªå®šä¹‰å®ç° |

---

## ğŸ¯ è¡ŒåŠ¨è®¡åˆ’ï¼ˆæ‰‹æœºAgentä¼˜åŒ–è·¯çº¿å›¾ï¼‰

### æœ¬æœˆ (2æœˆ)

- [ ] å¤ç°EdgeNav-QEåŸºç¡€å®éªŒï¼ˆ4-bit + Early Exitï¼‰
- [ ] åœ¨ScreenSpot-V2åŸºå‡†æµ‹è¯•
- [ ] è®°å½•baseline (FP16) å’Œä¼˜åŒ–åå¯¹æ¯”
- [ ] æ‰‹æœºç«¯éƒ¨ç½²éªŒè¯ï¼ˆSnapdragonæµ‹è¯•æœºï¼‰

### ä¸‹æœˆ (3æœˆ)

- [ ] å¼•å…¥Grouped Query Attentionï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
- [ ] ä¼˜åŒ–Early Exité˜ˆå€¼ï¼ˆé’ˆå¯¹GUIä»»åŠ¡ï¼‰
- [ ] åŠŸè€—æµ‹è¯•ï¼ˆç”µæ± æ¶ˆè€—ï¼‰
- [ ] å¤šè®¾å¤‡å…¼å®¹æ€§æµ‹è¯•

### Q2 (4-6æœˆ)

- [ ] å°è¯•SmoothQuantï¼ˆéœ€è¦é‡æ–°è®­ç»ƒï¼‰
- [ ] æ¢ç´¢MoEæ¶æ„ï¼ˆäº‘ç«¯å¤æ‚ä»»åŠ¡ï¼‰
- [ ] è‡ªåŠ¨åŒ–ä¼˜åŒ–é…ç½®ï¼ˆæ ¹æ®è®¾å¤‡è‡ªåŠ¨é€‰æ‹©æ–¹æ¡ˆï¼‰

---

## ğŸ’¡ æ ¸å¿ƒæ´å¯Ÿ

1. **æ²¡æœ‰é“¶å¼¹**: éœ€è¦ç»„åˆå¤šç§æŠ€æœ¯ï¼ˆé‡åŒ– + æ—©é€€ + GQAï¼‰
2. **ç²¾åº¦-æ•ˆç‡æƒè¡¡**: æ¯é¡¹æŠ€æœ¯éƒ½æœ‰ä»£ä»·ï¼Œéœ€æ‰¾åˆ°sweet spot
3. **ç¡¬ä»¶æ„ŸçŸ¥**: N:Mç¨€ç–ä¾èµ–A100ï¼Œæ‰‹æœºä¸è€ƒè™‘
4. **æ¨¡å‹é€‰æ‹©**: ä¼˜å…ˆé€‰æ”¯æŒGQAçš„æ¨¡å‹ï¼ˆQwen, Llama 2ï¼‰
3. **ä»»åŠ¡å®šåˆ¶**: Early Exitå¯¹GUIä»»åŠ¡ç‰¹åˆ«æœ‰æ•ˆï¼ˆç®€å•ç•Œé¢å¤šï¼‰

---

## ğŸ“š ç›¸å…³ç¬”è®°é“¾æ¥

- **20260220-011**: EdgeNav-QE (QLoRA + Early Exit)
- **20260220-009**: STEP3-VL-10B (10Bæ¨¡å‹æ¶æ„å‚è€ƒ)
- **20260220-002**: LLM AgentåŸºç¡€ (éœ€è¦ä¼˜åŒ–éƒ¨ç½²)

---

**æ€»ç»“**: Lilian Wengçš„è¿™ç¯‡ç»¼è¿°æ˜¯**Transformeræ¨ç†ä¼˜åŒ–çš„åœ£ç»**ã€‚å¯¹äºæ‰‹æœºAgenté¡¹ç›®ï¼Œæ ¸å¿ƒæ˜¯ **4-bité‡åŒ– + Early Exit + GQA** ä¸‰ä»¶å¥—ã€‚ç›®æ ‡ï¼š3-4GBå†…å­˜ã€<500mså»¶è¿Ÿã€>90%å‡†ç¡®ç‡ã€‚

---

*Created: 2026-02-20 16:10 | Quality: 0.94 | TODO: lea-??? (æ¨ç†ä¼˜åŒ–ä»»åŠ¡)*
