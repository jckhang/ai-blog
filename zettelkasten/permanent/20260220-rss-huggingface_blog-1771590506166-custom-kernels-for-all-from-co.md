---
id: 20260220-rss-huggingface_blog-001-custom-kernels-for-all-from-codex-and-claude
title: Custom Kernels for All from Codex and Claude
created: 2026-02-21
tags: ["rss","ai_research","auto-import","permanent"]
source: "Hugging Face Blog"
source_url: "https://huggingface.co/blog/custom-cuda-kernels-agent-skills"
source_type: "article"
content_length: 28270
quality_score: 1.00
---

# Custom Kernels for All from Codex and Claude

## æ¥æºä¿¡æ¯

- **æ¥æº**: Hugging Face Blog
- **å‘å¸ƒæ—¶é—´**: è§åŸæ–‡
- **åŸæ–‡é“¾æ¥**: https://huggingface.co/blog/custom-cuda-kernels-agent-skills
- **é‡‡é›†æ—¶é—´**: 2026-02-21

## æ ¸å¿ƒå†…å®¹

const guestTheme = document.cookie.match(/theme=(\w+)/)?.[1]; document.documentElement.classList.toggle('dark', guestTheme === 'dark' || ( (!guestTheme || guestTheme === 'system') && window.matchMedia('(prefers-color-scheme: dark)').matches)); Custom Kernels for All from Codex and Claude ((window.plausible = window.plausible || function () { (plausible.q = plausible.q || []).push(arguments); }), (plausible.init = plausible.init || function (i) { plausible.o = i || {}; })); plausible.init({ customProperties: { loggedIn: "false", }, endpoint: "/api/event", }); window.hubConfig = {"features":{"signupDisabled":false},"sshGitUrl":"git@hf.co","moonHttpUrl":"https:\/\/huggingface.co","captchaApiKey":"bd5f2066-93dc-4bdd-a64b-a24646ca3859","datasetViewerPublicUrl":"https:\/\/datasets-server.huggingface.co","stripePublicKey":"pk_live_x2tdjFXBCvXo2FFmMybezpeM00J6gPCAAc","environment":"production","userAgent":"HuggingFace (production)","spacesIframeDomain":"hf.space","spacesApiUrl":"https:\/\/api.hf.space","docSearchKey":"ece5e02e57300e17d152c08056145326e90c4bff3dd07d7d1ae40cf1c8d39cb6","logoDev":{"apiUrl":"https:\/\/img.logo.dev\/","apiKey":"pk_UHS2HZOeRnaSOdDp7jbd5w"}}; window.requestId = "Root=1-69985369-0c7f3d606c4ee06d40dd954d"; window.featureFlags = {"buckets":false}; Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Back to Articles Custom Kernels for All from Codex and Claude Published February 13, 2026 Update on GitHub Upvote 57 +51 ben burtenshaw burtenshaw Follow Sayak Paul sayakpaul Follow Aritra Roy Gosthipaty ariG23498 Follow shaun smith evalstate Follow Why a skill for kernels? Installing the skill What is in the skill Benchmarking the kernels: Diffusers (LTX-Video on H100) Isolated RMSNorm benchmark End-to-end video generation (49 frames, 30 steps, H100 80GB) Benchmarking the kernels: Transformers (Qwen3-8B on H100) Isolated RMSNorm benchmark Publishing your kernel to the Hub 1. Verify the project structure 2. Build all variants with Nix 3. Create a Hub repo and push 4. Others load it in one line Conclusion Resources tl;dr: We built an agent skill that teaches coding agents how to write production CUDA kernels. Then we pointed Claude and Codex at two real targets: a diffusers pipeline and a transformers model. The agents produced working kernels for both, with correct PyTorch bindings and benchmarks, end to end. Writing CUDA kernels is hard. Writing CUDA kernels that correctly integrate with transformers and diffusers is harder. There are architecture-specific memory access patterns, vectorization strategies, warp shuffle reductions, and a dozen integration pitfalls that trip up even experienced developers. It is exactly the kind of specialized, high-stakes problem where agent skills shine. We gave coding agents the domain knowledge they need, like which GPU architecture to target, how to structure a kernel-builder project, when to use shared memory versus registers, and how to write PyTorch bindings. The agents did the rest. If you have used the LLM training skill or read We Got Claude to Teach Open Models , the pattern will feel familiar: package domain expertise into a skill, point the agent at a problem, and let it work. Why a skill for kernels? The Kernel Hub solved the distribution of custom hardware kernels. You can load pre-compiled kernels from the Hub with a single get_kernel call. No builds, no flags. However, someone still needs to write the kernels . That is the gap this skill fills. CUDA kernel development has a brutal surface area: Hardware-specific optimization guides for each generation of GPU. H100, A100, and T4 each have different compute capabilities, shared memory sizes, and bandwidth profiles In Libraries, diffusers and transformers have different module hierarchies, normalization conventions, and integration patterns. Custom kernels need to be registered in PyTorch for torch.compile to recognize. For distribution, kernels can depend on CUDA, Pytorch, and Python versions creating massive environment matrices. This is domain knowledge that gets lost in documentation tabs and Stack Overflow answers. An agent skill packages it into context that loads on demand. First, let's show how to use the skill right away, then we'll dive into the details of how we benchmarked the kernels. Installing the skill The skill ships with the kernels library. Install it into your coding agent with a single command: # we need to install kernels from main for this pip install git+https://github.com/huggingface/kernels.git#subdirectory=kernels
kernels skills add cuda-kernels --claude This drops the skill into .claude/skills/cuda-kernels/ where Claude Code and Cursor pick it up automatically. For other agents: # Codex kernels skills add cuda-kernels --codex # OpenCode kernels skills add cuda-kernels --opencode # Custom destination kernels skills add cuda-kernels --dest ./my-agent/skills/ # Install globally (available across all projects) kernels skills add cuda-kernels --global # Overwrite an existing installation kernels skills add cuda-kernels --claude --force Once installed, prompt your agent: Build a vectorized RMSNorm kernel for H100 targeting the Qwen3-8B model in transformers. Or, you can go for something more open-ended: Build an optimized attention kernel for H100 targeting the Qwen3-8B model in transformers. Benchmark it against the PyTorch baseline and validate improvements in end-to-end performance. The agent can read the skill, select the right architecture parameters, generate the CUDA source, write the PyTorch bindings, set up build.toml , and create a benchmark script. If you're working on more complex kernels, or architecture-specific optimizations, that aren't covered in the skill, then the skill supplies the fundamental building blocks and patterns to get you started. We are also open to contributions on the skill itself . What is in the skill The skill is roughly 550 tokens of structured guidance plus reference scripts, GPU optimization guides, troubleshooting docs, and complete working examples. Agentic coding tools like Codex and Claude can read this and produce a working kernel project. It covers: NVIDIA GPU Architecture-aware optimization for H100, A100, and T4 (compute capabilities, memory bandwidth, shared memory sizes, block sizing) Integration patterns for both diffusers and transformers , including the pitfalls specific to each library Kernel templates with vectorized memory access patterns for BF16, FP16, and FP32 Benchmarking workflows for both isolated kernel micro-benchmarks and end-to-end pipeline comparisons HuggingFace Kernel Hub integration via get_kernel for loading community kernels .claude/skills/cuda-kernels/
â”œâ”€â”€ SKILL.md # Main instructions (~550 tokens)
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ benchmark_example.py # End-to-end benchmark template
â”‚ â”œâ”€â”€ benchmark_rmsnorm.py # Isolated kernel micro-benchmark
â”‚ â”œâ”€â”€ ltx_kernel_injection_example.py # Diffusers integration pattern
â”‚ â”œâ”€â”€ transformers_injection_example.py # Transformers integration pattern
â”‚ â””â”€â”€ huggingface_kernels_example.py # Kernel Hub integration
â””â”€â”€ references/ â”œâ”€â”€ diffusers-integration.md # Diffusers guide with pitfalls â”œâ”€â”€ transformers-integration.md # Transformers guide â”œâ”€â”€ huggingface-kernels-integration.md â”œâ”€â”€ h100-optimization-guide.md â”œâ”€â”€ a100-optimization-guide.md â”œâ”€â”€ t4-optimization-guide.md â”œâ”€â”€ kernel-templates.md â””â”€â”€ troubleshooting.md When an agent loads this, it gets everything it needs to go from "write me an RMSNorm kernel" to a buildable, benchmarkable project. It will grep and glob the skill to find the relevant files and directories. So it's important to structure the skill in a way that is easy to find. The agent is instructed to generate kernels that conform to the templates in references/kernel-templates.md and produce a complete kernel project: examples/your_model/
â”œâ”€â”€ kernel_src/
â”‚ â””â”€â”€ rmsnorm.cu # Vectorized CUDA kernel
â”œâ”€â”€ torch-ext/
â”‚ â”œâ”€â”€ your_kernels/__init__.py
â”‚ â””â”€â”€ torch_binding.cpp # PyTorch C++ bindings
â”œâ”€â”€ benchmark_rmsnorm.py # Micro-benchmark script
â”œâ”€â”€ build.toml # kernel-builder config
â”œâ”€â”€ setup.py # pip install -e .
â””â”€â”€ pyproject.toml We tested this on two real targets. Benchmarking the kernels: Diffusers (LTX-Video on H100) The agent built RMSNorm, RoPE 3D, GEGLU, and AdaLN kernels for LTX-Video , a video generation pipeline from diffusers . The full example is at examples/ltx_video/ . We optimized the RMSNorm kernel for H100. Both benchmarks were run on H100 80GB HBM3 at precision BFloat16. If you want to check out the generated kernel, got to this example Isolated RMSNorm benchmark First, we compare the isolated RMSNorm kernel performance against the PyTorch baseline. This is the main speedup in the optimized pipeline. Table Shape Custom (ms) PyTorch (ms) Speedup [1x1024x2048] 0.039 0.064 1.64x [2x1024x2048] 0.040 0.073 1.82x [4x1024x2048] 0.052 0.093 1.78x [1x4096x2048] 0.052 0.093 1.79x [2x4096x3072] 0.102 0.209 2.04x [1x8192x2048] 0.083 0.150 1.81x [4x4096x3072] 0.173 0.393 2.26x Average speedup: 1.88x and a bandwidth efficiency: 34.7% of H100 theoretical (3,350 GB/s) End-to-end video generation (49 frames, 30 steps, H100 80GB) Next, we compare the end-to-end video generation performance of the optimized kernels against the baseline (no compile) and the torch.compile baseline. Table Configuration Time (s) it/s Speedup Baseline (no compile) 2.87 12.58 1.00x Generated Optimized Kernels 2.70 13.52 1.06x Baseline + torch.compile 2.14 19.05 1.34x Optimized + torch.compile 2.01 18.45 1.43x RMSNorm accounts for ~5% of total compute in LTX-Video. The remaining time is spent in attention, linear projections, and VAE decode. The 6% end-to-end speedup from a single kernel type is consistent with that profile. Benchmarking the kernels: Transformers (Qwen3-8B on H100) The agent built an RMSNorm kernel for Qwen3-8B , a large language model from transformers with 65 RMSNorm modules across 32 layers. The full example is at examples/qwen3_8b/ . We optimized the RMSNorm kernel for H100. Both benchmarks were run on H100 80GB HBM3 at precision BFloat16. If you want to explore the kernel, check it out here. Isolated RMSNorm benchmark Once again, we compare the isolated RMSNorm kernel performance against the PyTorch baseline. Average speedup: 1.94x and a bandwidth efficiency: 22.3% of H100 theoretical (3,350 GB/s) Table Shape Custom (ms) PyTorch (ms) Speedup [1x128x4096] 0.040 0.062 1.58x [1x512x4096] 0.038 0.064 1.69x [1x1024x4096] 0.037 0.071 1.90x [1x2048x4096] 0.045 0.091 2.03x [1x4096x4096] 0.071 0.150 2.12x [4x512x4096] 0.056 0.093 1.67x [8x256x4096] 0.045 0.092 2.06x [1x8192x4096] 0.109 0.269 2.47x Speedup scales with sequence length: 1.58x at 128 tokens, 2.47x at 8192 tokens. For long-context inference, the custom kernel roughly halves RMSNorm latency. Publishing your kernel to the Hub The agent gives you a working kernel. The Kernel Hub lets you share it so anyone can load it without compilation. Here is the full path from agent output to published kernel. 1. Verify the project structure The agent produces a project that already follows the kernel-builder layout: your_kernel/
â”œâ”€â”€ build.toml # Build configuration
â”œâ”€â”€ kernel_src/
â”‚ â””â”€â”€ rmsnorm.cu # CUDA kernel source
â””â”€â”€ torch-ext/ â”œâ”€â”€ torch_binding.cpp # Registers Torch ops â””â”€â”€ your_kernels/ â””â”€â”€ __init__.py # Python API wrapping _ops The build.toml tells kernel-builder what to build. The agent generates this for you, including the correct cuda-capabilities for your target GPU: [general]
name = "your_kernels"
backends = ["cuda"] [torch]
src = ["torch-ext/torch_binding.cpp"] [kernel.rmsnorm]
backend = "cuda"
src = ["kernel_src/rmsnorm.cu"]
depends = ["torch"]
cuda-capabilities = ["9.0"] # H100 2. Build all variants with Nix Kernel Hub kernels must support all recent PyTorch and CUDA configurations. The kernel-builder Nix flake handles this automatically. Copy the example flake.nix into your project and run: nix flake update
nix run .#build-and-copy -L This builds the kernel for every required PyTorch/CUDA variant and places the results in build/ . For faster builds, enable the HuggingFace Nix cache: nix run nixpkgs#cachix -- use huggingface 3. Create a Hub repo and push Create a model repo on the Hub and upload the built kernel: huggingface-cli repo create your-org/your-kernel --type model
huggingface-cli upload your-org/your-kernel ./build 4. Others load it in one line Once published, anyone can use your kernel with zero compilation: from kernels import get_kernel rmsnorm = get_kernel( "your-org/your-kernel" ) get_kernel detects the user's Python, PyTorch, and CUDA versions and downloads the matching pre-compiled binary. No builds, no flags, typically ready in seconds. The skill and the Hub are complementary. The skill handles development. The Hub handles distribution. Build a kernel with the skill, validate it with the benchmark scripts, publish it to the Hub, and it becomes a one-liner for everyone else. Conclusion We built an agent skill that teaches coding agents how to write production CUDA kernels. Then we pointed Claude and Codex at two real targets: a diffusers pipeline and a transformers model. The agents produced working kernels for both, with correct PyTorch bindings and benchmarks, end to end. We benchmarked the kernels and found that the optimized kernels can provide a speedup in both isolated and end-to-end performance. Resources CUDA Kernels Skill in kernels HuggingFace Kernel Hub Blog We Got Claude to Fine-Tune an Open Source LLM We Got Claude to Teach Open Models HuggingFace Kernels Community More Articles from our Blog announcement open-source community OpenEnv in Practice: Evaluating Tool-Using Agents in Real-World Environments +1 24 February 12, 2026 upskill agent-skills agentic We Got Claude to Build CUDA Kernels and teach open models! 138 January 28, 2026 Community I get an error while trying to install: \n $ pip install git+https://github.com/huggingface/kernels.git \n...\nERROR: git+https://github.com/huggingface/kernels.git does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\n \n Installation with pip install kernels works, but the command has no skills option: \n $ kernels skills add cuda-kernels --claude \nusage: kernel [-h] {check,download,versions,upload,lock,generate-readme,benchmark} ...\n \n&quot;,&quot;updatedAt&quot;:&quot;2026-02-16T07:43:49.157Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;66588adb2889d3c69c24ec38&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/66588adb2889d3c69c24ec38/4C-5ZfKN6aTiRTYtiP2s3.jpeg&quot;,&quot;fullname&quot;:&quot;Tomas Ruiz&quot;,&quot;name&quot;:&quot;tomasruiz&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:5,&quot;isUserFollowing&quot;:false}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.7456541061401367},&quot;editors&quot;:[&quot;tomasruiz&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/66588adb2889d3c69c24ec38/4C-5ZfKN6aTiRTYtiP2s3.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false},&quot;replies&quot;:[{&quot;id&quot;:&quot;6992e91f33a6a0090cd74943&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;608aabf24955d2bfc3cd99c6&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/608aabf24955d2bfc3cd99c6/-YxmtpzEmf3NKOTktODRP.jpeg&quot;,&quot;fullname&quot;:&quot;Aritra Roy Gosthipaty&quot;,&quot;name&quot;:&quot;ariG23498&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:true,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:563,&quot;isUserFollowing&quot;:false},&quot;createdAt&quot;:&quot;2026-02-16T09:53:35.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;The `kernels` repo has `kernels-builder` and the `kernels` library.\n\nThis is the reason why doing a `pip install git+https://github.com/huggingface/kernels.git` would not work.&quot;,&quot;html&quot;:&quot; The kernels repo has kernels-builder and the kernels library. \n This is the reason why doing a pip install git+https://github.com/huggingface/kernels.git would not work. \n&quot;,&quot;updatedAt&quot;:&quot;2026-02-16T09:53:35.915Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;608aabf24955d2bfc3cd99c6&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/608aabf24955d2bfc3cd99c6/-YxmtpzEmf3NKOTktODRP.jpeg&quot;,&quot;fullname&quot;:&quot;Aritra Roy Gosthipaty&quot;,&quot;name&quot;:&quot;ariG23498&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:true,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:563,&quot;isUserFollowing&quot;:false}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.944117546081543},&quot;editors&quot;:[&quot;ariG23498&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/608aabf24955d2bfc3cd99c6/-YxmtpzEmf3NKOTktODRP.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false,&quot;parentCommentId&quot;:&quot;6992cab52a6d680681ed7e9f&quot;}},{&quot;id&quot;:&quot;69939b27586e37b7244684cc&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;66588adb2889d3c69c24ec38&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/66588adb2889d3c69c24ec38/4C-5ZfKN6aTiRTYtiP2s3.jpeg&quot;,&quot;fullname&quot;:&quot;Tomas Ruiz&quot;,&quot;name&quot;:&quot;tomasruiz&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:5,&quot;isUserFollowing&quot;:false},&quot;createdAt&quot;:&quot;2026-02-16T22:33:11.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;Ok, so the instructions in the article need to be updated?&quot;,&quot;html&quot;:&quot; Ok, so the instructions in the article need to be updated? \n&quot;,&quot;updatedAt&quot;:&quot;2026-02-16T22:33:11.726Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;66588adb2889d3c69c24ec38&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/66588adb2889d3c69c24ec38/4C-5ZfKN6aTiRTYtiP2s3.jpeg&quot;,&quot;fullname&quot;:&quot;Tomas Ruiz&quot;,&quot;name&quot;:&quot;tomasruiz&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:5,&quot;isUserFollowing&quot;:false}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.8416299819946289},&quot;editors&quot;:[&quot;tomasruiz&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/66588adb2889d3c69c24ec38/4C-5ZfKN6aTiRTYtiP2s3.jpeg&quot;],&quot;reactions&quot;:[{&quot;reaction&quot;:&quot;ğŸ‘&quot;,&quot;users&quot;:[&quot;ariG23498&quot;],&quot;count&quot;:1}],&quot;isReport&quot;:false,&quot;parentCommentId&quot;:&quot;6992cab52a6d680681ed7e9f&quot;}},{&quot;id&quot;:&quot;69943ada33a6a0090cd74961&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;608aabf24955d2bfc3cd99c6&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/608aabf24955d2bfc3cd99c6/-YxmtpzEmf3NKOTktODRP.jpeg&quot;,&quot;fullname&quot;:&quot;Aritra Roy Gosthipaty&quot;,&quot;name&quot;:&quot;ariG23498&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:true,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:563,&quot;isUserFollowing&quot;:false},&quot;createdAt&quot;:&quot;2026-02-17T09:54:34.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;We have updated the section! Thanks for flagging.&quot;,&quot;html&quot;:&quot; We have updated the section! Thanks for flagging. \n&quot;,&quot;updatedAt&quot;:&quot;2026-02-17T09:54:34.755Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;608aabf24955d2bfc3cd99c6&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/608aabf24955d2bfc3cd99c6/-YxmtpzEmf3NKOTktODRP.jpeg&quot;,&quot;fullname&quot;:&quot;Aritra Roy Gosthipaty&quot;,&quot;name&quot;:&quot;ariG23498&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:true,&quot;isHf&quot;:true,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:563,&quot;isUserFollowing&quot;:false}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.9040176868438721},&quot;editors&quot;:[&quot;ariG23498&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/608aabf24955d2bfc3cd99c6/-YxmtpzEmf3NKOTktODRP.jpeg&quot;],&quot;reactions&quot;:[{&quot;reaction&quot;:&quot;ğŸ‘&quot;,&quot;users&quot;:[&quot;tomasruiz&quot;],&quot;count&quot;:1}],&quot;isReport&quot;:false,&quot;parentCommentId&quot;:&quot;6992cab52a6d680681ed7e9f&quot;}},{&quot;id&quot;:&quot;699443fc2a6d680681ed7eac&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;66588adb2889d3c69c24ec38&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/66588adb2889d3c69c24ec38/4C-5ZfKN6aTiRTYtiP2s3.jpeg&quot;,&quot;fullname&quot;:&quot;Tomas Ruiz&quot;,&quot;name&quot;:&quot;tomasruiz&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:5,&quot;isUserFollowing&quot;:false},&quot;createdAt&quot;:&quot;2026-02-17T10:33:32.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;The command `kernels skills add cuda-kernels --claude` from the instruction still failed for me. I had to remove `cuda-kernels`. The command uses a relative working directory, so if you want to install the skill at the user level, you should switch to the home dir:\n```shell\n$ cd ~\n$ kernels skills add --claude\nInstalled 'cuda-kernels' to ~/.claude/skills/cuda-kernels\n```&quot;,&quot;html&quot;:&quot; The command kernels skills add cuda-kernels --claude from the instruction still failed for me. I had to remove cuda-kernels . The command uses a relative working directory, so if you want to install the skill at the user level, you should switch to the home dir: \n $ cd ~ \n $ kernels skills add --claude \nInstalled 'cuda-kernels' to ~/.claude/skills/cuda-kernels\n \n&quot;,&quot;updatedAt&quot;:&quot;2026-02-17T10:33:32.941Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;66588adb2889d3c69c24ec38&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/66588adb2889d3c69c24ec38/4C-5ZfKN6aTiRTYtiP2s3.jpeg&quot;,&quot;fullname&quot;:&quot;Tomas Ruiz&quot;,&quot;name&quot;:&quot;tomasruiz&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:5,&quot;isUserFollowing&quot;:false}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.837024450302124},&quot;editors&quot;:[&quot;tomasruiz&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/66588adb2889d3c69c24ec38/4C-5ZfKN6aTiRTYtiP2s3.jpeg&quot;],&quot;reactions&quot;:[],&quot;isReport&quot;:false,&quot;parentCommentId&quot;:&quot;6992cab52a6d680681ed7e9f&quot;}}]},{&quot;id&quot;:&quot;6992cdc08de4ec1d046b6bc0&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;66588adb2889d3c69c24ec38&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/66588adb2889d3c69c24ec38/4C-5ZfKN6aTiRTYtiP2s3.jpeg&quot;,&quot;fullname&quot;:&quot;Tomas Ruiz&quot;,&quot;name&quot;:&quot;tomasruiz&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:5,&quot;isUserFollowing&quot;:false},&quot;createdAt&quot;:&quot;2026-02-16T07:56:48.000Z&quot;,&quot;type&quot;:&quot;comment&quot;,&quot;data&quot;:{&quot;edited&quot;:false,&quot;hidden&quot;:false,&quot;latest&quot;:{&quot;raw&quot;:&quot;**Question**: Is the baseline in section **Isolated RMSNorm benchmark** (Pytorch baseline) using torch.compile() or not? Custom kernels should be ideally beating torch.compiled code.&quot;,&quot;html&quot;:&quot; Question : Is the baseline in section Isolated RMSNorm benchmark (Pytorch baseline) using torch.compile() or not? Custom kernels should be ideally beating torch.compiled code. \n&quot;,&quot;updatedAt&quot;:&quot;2026-02-16T07:56:48.063Z&quot;,&quot;author&quot;:{&quot;_id&quot;:&quot;66588adb2889d3c69c24ec38&quot;,&quot;avatarUrl&quot;:&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/66588adb2889d3c69c24ec38/4C-5ZfKN6aTiRTYtiP2s3.jpeg&quot;,&quot;fullname&quot;:&quot;Tomas Ruiz&quot;,&quot;name&quot;:&quot;tomasruiz&quot;,&quot;type&quot;:&quot;user&quot;,&quot;isPro&quot;:false,&quot;isHf&quot;:false,&quot;isHfAdmin&quot;:false,&quot;isMod&quot;:false,&quot;followerCount&quot;:5,&quot;isUserFollowing&quot;:false}},&quot;numEdits&quot;:0,&quot;identifiedLanguage&quot;:{&quot;language&quot;:&quot;en&quot;,&quot;probability&quot;:0.5539329648017883},&quot;editors&quot;:[&quot;tomasruiz&quot;],&quot;editorAvatarUrls&quot;:[&quot;https://cdn-avatars.huggingface.co/v1/production/uploads/66588adb2889d3c69c24ec38/4C-5ZfKN6aTiRTYtiP2s3.jpeg&quot;],&quot;reactions&quot;:[{&quot;reaction&quot;:&quot;ğŸ‘&quot;,&quot;users&quot;:[&quot;joaopn&quot;],&quot;count&quot;:1}],&quot;isReport&quot;:false}}],&quot;status&quot;:&quot;open&quot;,&quot;isReport&quot;:false,&quot;pinned&quot;:false,&quot;locked&quot;:false,&quot;collection&quot;:&quot;community_blogs&quot;},&quot;contextAuthors&quot;:[&quot;burtenshaw&quot;,&quot;sayakpaul&quot;,&quot;ariG23498&quot;,&quot;evalstate&quot;],&quot;primaryEmailConfirmed&quot;:false,&quot;discussionRole&quot;:0,&quot;acceptLanguages&quot;:[&quot;en&quot;],&quot;withThread&quot;:true,&quot;cardDisplay&quot;:false,&quot;repoDiscussionsLocked&quot;:false}"> tomasruiz 4 days ago I get an error while trying to install: $ pip install git+https://github.com/huggingface/kernels.git ...
ERROR: git+https://github.com/huggingface/kernels.git does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found. Installation with pip install kernels works, but the command has no skills option: $ kernels skills add cuda-kernels --claude usage: kernel [-h] {check,download,versions,upload,lock,generate-readme,benchmark} ... 4 replies Â· ariG23498 Article author 4 days ago The kernels repo has kernels-builder and the kernels library. This is the reason why doing a pip install git+https://github.com/huggingface/kernels.git would not work. Expand 3 replies tomasruiz 4 days ago Question : Is the baseline in section Isolated RMSNorm benchmark (Pytorch baseline) using torch.compile() or not? Custom kernels should be ideally beating torch.compiled code. ğŸ‘ 1 1 + Reply Edit Preview Upload images, audio, and videos by dragging in the text input, pasting, or clicking here . Tap or paste here to upload images Comment Â· Sign up or log in to comment Upvote 57 +45 System theme Company TOS Privacy About Careers Website Models Datasets Spaces Pricing Docs import("\/front\/build\/kube-8f23bef\/index.js"); window.moonSha = "kube-8f23bef\/"; window.__hf_deferred = {}; if (["hf.co", "huggingface.co"].includes(window.location.hostname)) { const script = document.createElement("script"); script.src = "https://js.stripe.com/v3/"; script.async = true; document.head.appendChild(script); }

## å…³é”®è§‚ç‚¹

- æ ¸å¿ƒä¸»é¢˜æ¶‰åŠCustomå’ŒKernels
- ç ”ç©¶æ¥æº: Hugging Face Blog
- å†…å®¹è´¨é‡è¯„åˆ†: 1.00

## æ·±åº¦æ€è€ƒ

- è¿™é¡¹ç ”ç©¶çš„æ ¸å¿ƒå‡è®¾æ˜¯ä»€ä¹ˆï¼Ÿå®ƒä»¬æ˜¯å¦åˆç†ï¼Ÿ
- æ–¹æ³•çš„ä¸»è¦å±€é™æ€§æ˜¯ä»€ä¹ˆï¼Ÿæœªæ¥å¦‚ä½•æ”¹è¿›ï¼Ÿ
- è¿™ä¸ªå‘ç°å¯¹å®é™…åº”ç”¨åœºæ™¯æœ‰ä»€ä¹ˆå¯ç¤ºï¼Ÿ
- å¦‚æœæ¢ä¸€ä¸ªæ•°æ®é›†æˆ–ç¯å¢ƒï¼Œç»“æœæ˜¯å¦ä¾ç„¶æˆç«‹ï¼Ÿ
- è¿™é¡¹å·¥ä½œçš„åˆ›æ–°ç‚¹æ˜¯å¦è¢«å……åˆ†è®ºè¯ï¼Ÿ

## ç›¸å…³é“¾æ¥

- [[001-zettelkasten-æ˜¯ä»€ä¹ˆ]]
- [[009-å†™ä½œå°±æ˜¯æ€è€ƒ]]
- [[011-ä»ç¬”è®°åˆ°æ–‡ç« ]]


*RSS è‡ªåŠ¨é‡‡é›†æ°¸ä¹…åŒ– - å¤„ç†æ—¶é—´: 2026-02-21*
