---
id: 20260219-rss-lilian_weng-001-thinking-about-high-quality-human-data
title: Thinking about High-Quality Human Data
created: 2026-02-21
tags: ["rss","engineering","auto-import","permanent"]
source: "Lilian Weng's Blog"
source_url: "https://lilianweng.github.io/posts/2024-02-05-human-data-quality/"
source_type: "article"
content_length: 1018
quality_score: 0.75
---

# Thinking about High-Quality Human Data

## æ¥æºä¿¡æ¯

- **æ¥æº**: Lilian Weng's Blog
- **å‘å¸ƒæ—¶é—´**: è§åŸæ–‡
- **åŸæ–‡é“¾æ¥**: https://lilianweng.github.io/posts/2024-02-05-human-data-quality/
- **é‡‡é›†æ—¶é—´**: 2026-02-21

## æ ¸å¿ƒå†…å®¹

<span class="update">[Special thank you to [Ian Kivlichan](https://scholar.google.com/citations?user=FRBObOwAAAAJ&hl=en) for many useful pointers (E.g. the 100+ year old Nature paper &ldquo;Vox populi&rdquo;) and nice feedback. ğŸ™ ]</span>

High-quality data is the fuel for modern data deep learning model training. Most of the task-specific labeled data comes from human annotation, such as classification task or [RLHF](https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#rl-fine-tuning-with-human-preferences) labeling (which can be constructed as classification format) for LLM alignment training. Lots of ML techniques in the post can help with data quality, but fundamentally human data collection involves attention to details and careful execution. The community knows the value of high quality data, but somehow we have this subtle impression that â€œEveryone wants to do the model work, not the data workâ€ ([Sambasivan et al. 2021](https://dl.acm.org/doi/abs/10.1145/3411764.3445518)).

## å…³é”®è§‚ç‚¹

- æ ¸å¿ƒä¸»é¢˜æ¶‰åŠThinkingå’ŒHigh-Quality
- ç ”ç©¶æ¥æº: Lilian Weng's Blog
- å†…å®¹è´¨é‡è¯„åˆ†: 0.75

## æ·±åº¦æ€è€ƒ

<!-- å¯¹ä¸Šè¿°è§‚ç‚¹çš„åæ€ã€è´¨ç–‘ã€å»¶ä¼¸é—®é¢˜ -->

## ç›¸å…³é“¾æ¥

- [[016-llm-research-automation]]
- [[017-æ·±åº¦ç ”ç©¶å·¥å…·é“¾]]
- [[018-ç ”ç©¶æ‰«æè‡ªåŠ¨åŒ–çš„ZKé›†æˆç­–ç•¥]]

---
*RSS è‡ªåŠ¨é‡‡é›†æ°¸ä¹…åŒ– - å¤„ç†æ—¶é—´: 2026-02-21*
