---
id: 20260219-rss-lilian_weng-001-diffusion-models-for-video-generation
title: Diffusion Models for Video Generation
created: 2026-02-21
tags: ["rss","engineering","auto-import","permanent"]
source: "Lilian Weng's Blog"
source_url: "https://lilianweng.github.io/posts/2024-04-12-diffusion-video/"
source_type: "article"
content_length: 946
quality_score: 0.60
---

# Diffusion Models for Video Generation

## æ¥æºä¿¡æ¯

- **æ¥æº**: Lilian Weng's Blog
- **å‘å¸ƒæ—¶é—´**: è§åŸæ–‡
- **åŸæ–‡é“¾æ¥**: https://lilianweng.github.io/posts/2024-04-12-diffusion-video/
- **é‡‡é›†æ—¶é—´**: 2026-02-21

## æ ¸å¿ƒå†…å®¹

[Diffusion models](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task&mdash;using it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:

<ol>
- It has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.

- In comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.

</ol>
<blockquote>

<b>
ğŸ¥‘ Required Pre-read: Please make sure you have read the previous blog on [&ldquo;What are Diffusion Models?&rdquo;](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) for image generation before continue here.
</b>

## å…³é”®è§‚ç‚¹

- æ ¸å¿ƒä¸»é¢˜æ¶‰åŠDiffusionå’ŒModels
- ç ”ç©¶æ¥æº: Lilian Weng's Blog
- å†…å®¹è´¨é‡è¯„åˆ†: 0.60

## æ·±åº¦æ€è€ƒ

<!-- å¯¹ä¸Šè¿°è§‚ç‚¹çš„åæ€ã€è´¨ç–‘ã€å»¶ä¼¸é—®é¢˜ -->

## ç›¸å…³é“¾æ¥

- [[016-llm-research-automation]]
- [[017-æ·±åº¦ç ”ç©¶å·¥å…·é“¾]]
- [[018-ç ”ç©¶æ‰«æè‡ªåŠ¨åŒ–çš„ZKé›†æˆç­–ç•¥]]

---
*RSS è‡ªåŠ¨é‡‡é›†æ°¸ä¹…åŒ– - å¤„ç†æ—¶é—´: 2026-02-21*
