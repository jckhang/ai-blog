---
title: "Increasing AI Strategic Competence as a Safety Approach"
source: "ai-alignment RSS"
url: "https://www.alignmentforum.org/posts/uECnWtbQ95dDWqBKD/increasing-ai-strategic-competence-as-a-safety-approach"
date: 2026-02-03T01:08:13.000Z
tags: [rss, ai-alignment]
status: pending-analysis
---

# Increasing AI Strategic Competence as a Safety Approach

> Published on February 3, 2026 1:08 AM GMT<br/><br/><p>If AIs became strategically competent enough, they may realize that RSI is too dangerous because they're not good enough at alignment or philosophy or strategy, and potentially convince, help, or work with humans to implement an AI pause. This presents an alternative "victory condition" that someone could pursue (e.g. by working on AI strategic competence) if they were relatively confident about the alignment of near-human-level AIs but concerned about the AI transition as a whole, for example because they're worried about alignment of ASI, or worried about correctly solving other philosophical problems that would arise during the transition. (But note that if the near-human-level AIs are <i>not</i> aligned, then this effort could backfire by letting them apply better strategy to take over more easily.)</p><h3>Strategic vs Philosophical Competence</h3><p>The previous "victory path" I've been focused on was to improve AI philosophical competence, under the theory that if the AIs are aligned, they'll want to help us align the next generation of AIs and otherwise help guide us through the AI transition. I think by default they will be too incompetent at philosophical reasoning to do a good enough job at this, hence the proposal to improve such competence. However accomplishing this may well be <a href="https://www.lesswrong.com/posts/zFZHHnLez6k8ykxpu/building-ais-that-do-human-like-philosophy?commentId=8hKs6zarPuMBjmTmC">too hard</a>, thus leading to this new idea.&nbsp;</p><p>I note that high-level strategic competence shares some characteristics with philosophical competence, such as sparse or absent feedback from reality and dependence on human evaluations, but may be significantly easier due to more conceptual clarity about the target being aimed for, and continuity with other easier-to-train capabilities such as low and mid-level strategy.</p><h3>Unilateral Refusal vs AI Assistance for Pausing AI</h3><p>I found a couple of related posts, <a href="https://www.lesswrong.com/posts/CFA8W6WCodEZdjqYE/ais-should-also-refuse-to-work-on-capabilities-research">AIs should also refuse to work on capabilities research</a> by <a href="https://www.lesswrong.com/users/davidmanheim?mention=user">@Davidmanheim</a> and <a href="https://www.lesswrong.com/posts/fdCaCDfstHxyPmB9h/vladimir_nesov-s-shortform?commentId=sdCcxndCqXghfnFgc">this shortform by Vladimir Nesov</a>. There's also an <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4445706">earlier paper</a> that makes a similar point as David Manheim's post, which focuses on AIs unilaterally refusing to do capabilities research. But I think this has two issues:</p><ol><li>The AIs may not be strategically competent enough to decide to refuse, similar to how a large number of humans are <i>not</i> refusing to work on AI capabilities research.</li><li>Such unilateral refusal is a form of intent misalignment, and seems relatively easy for AI companies to "correct" or prevent by using standard control and/or alignment techniques. (<a href="https://www.lesswrong.com/posts/CFA8W6WCodEZdjqYE/ais-should-also-refuse-to-work-on-capabilities-research?commentId=8J8k3qHaQtCD9awrD">This comment</a> by <a href="https://www.lesswrong.com/users/tanae?mention=user">@tanae</a> makes a similar point.)</li></ol><p>In comparison, my "victory path" sees some humans working deliberately to increase AI strategic competence, and instead of unilaterally refusing to contribute to RSI, the AIs help or work with more humans (including by argumentation/persuasion/advice) to implement a global RSI pause.</p><br/><br/><a href="https://www.alignmentforum.org/posts/uECnWtbQ95dDWqBKD/increasing-ai-strategic-competence-as-a-safety-approach#comments">Discuss</a>

## Full Content

⚠️ 内容抓取延迟到分析阶段。使用 web_fetch(url) 或 Jina AI 补充。

## Analysis Checklist

- [ ] 阅读全文并提取关键观点
- [ ] 与现有 ZK 笔记建立链接
- [ ] 确定是否转为永久笔记
- [ ] 添加适当的标签和元数据
- [ ] 更新摘要

## Related

- 
