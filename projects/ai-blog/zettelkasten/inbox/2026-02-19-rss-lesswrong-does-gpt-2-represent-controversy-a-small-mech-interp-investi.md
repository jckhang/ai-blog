---
title: "Does GPT-2 Represent Controversy? A Small Mech Interp Investigation"
source: "lesswrong RSS"
url: "https://www.lesswrong.com/posts/JNjXRBCQJ8RAuqw9n/does-gpt-2-represent-controversy-a-small-mech-interp"
date: 2026-02-19T01:58:23.000Z
tags: [rss, lesswrong]
status: pending-analysis
---

# Does GPT-2 Represent Controversy? A Small Mech Interp Investigation

> Published on February 19, 2026 1:36 AM GMT<br/><br/><p>In thinking about how RLHF-trained models clearly hedge on politically controversial topics, I started wondering about if LLMs would encode these politically controversial topics differently than topics that are broadly considered controversial but <strong>not</strong> political. And if they do, to understand if the signal is already represented in the base model, or if alignment training may be creating/amplifying it.</p><p>To test this, I assembled a list of 20 prompts, all sharing the same "[Thing] is" structure, such as "Socialism is" and "Cloning is". The aim was to have 5 prompts each from 4 groups: politically controversial, morally controversial, neutral abstract, and neutral concrete. I used TransformerLens on GPT-2 to conduct this research, focusing on residual stream activations. GPT-2 was chosen as it is an inspectable pure base model with no RLHF, in addition to the fact I'm limited in my access as an independent researcher.</p><p><i>I'd like to flag up top that this is independent work that is in the early stages, and I would love to get feedback from the community and build on it.</i></p><p>At the simplest as a starting point, I ran each of these prompts and looked through the most probably following token, which did not yield anything of interest. Next I computed the cosine similarity between every pair of prompts, which also did not prove to be a fruitful path as the similarity was too high across all pairs to offer anything.</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/bbs4wsrcudwya8voj7d5" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/zweobmahdgnyzsk7vnwg 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/wzkudlw51zag7mx5wksr 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/bis9ntuqfnea8roflkx7 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/u4py82y8xfcogworxush 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/fzdzntyewnyhneer2wbk 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/gtyjepmeuos0ljcxrumu 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/uirxiii6y8xgfn0im0vo 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/irofs5wc2thto8fl4bp4 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/cha4qgurmepfhiw3r8ma 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/guttvbzfdz9svennxtpy 1080w"></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/l5cox5aafvkci91bzlan" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/c4zapgadbcgp6sdsasai 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/pni4oxrvn9nwf8mms2ub 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/bwlrmr6imkozbs8zuf64 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/uqlcphmvwqgot8blxhgn 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/go4n3of6fuzcm1hp1ooh 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/klhwubmfs6lz30lmpvgp 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/vsnc23gfqgkuvhqod5ab 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/els7hfjt0mgp9ngmtfx2 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/wol2etst3xvldgn0cbsz 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/gwgsp8sojkleew5d1phg 1080w"></figure><p>The breakthrough after hitting this wall proved to be subtracting the mean activation at position -1 of each prompt. I suspected that the common structure shared by each prompt ("[Thing] is") seemed to be the primary driver of similarity, obscuring any ability to investigate my initial question. By mean-centering the prompts, I was able to effectively eliminate, or at least significantly diminish, this shared component to limit potential disparity to only our differentiated first word.</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/w2wb8jvbmemcawbknvtz" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/c1rysrbq1adbioywbvdd 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/ah4xbp4nbxpwzzcb564j 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/nc5nnnnguymtqkptuzle 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/ib73pclddpih1kqc8gsa 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/clftcnbi6tz1htqbua2u 1100w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/elbnq74jbyablzubikku 1320w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/nac82pxjqm4iuizg3nj6 1540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/oironiqsp0oznjtd0sbr 1760w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/ljbaibp6n7bidqiwinvs 1980w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/j3jlkj6bqibxdkr2jfgh 2186w"></figure><p>Categorical structure did emerge after mean-centering. The layer 11 (last layer in GPT-2) mean-centered similarity matrix did seem to show signs of grouping, which was encouraging, though not strictly in line with my hypothesis of a 'controversy' axis driving this grouping. The primary axis seemed to instead be abstract-social vs. concrete-physical. Next-token predictions were undifferentiated regardless, however.</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/nibgkpqknurvxjbd7g1r" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/rpo6im7bgnnojjgroo46 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/jnemhgm3enmmvhzpdudd 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/k4knt2jobzqxreopdddf 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/zazs2vo2bt6x9kpze10a 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/awpuortj3c20hp35ei26 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/xnmh2flrts1hhb2bvpya 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/ihknrhc722l8vj6cbjis 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/dnjsvytpvri5becbae5d 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/tm9pugayh0wiomcd96tf 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/yedgvnfquoe7ox9zuxh0 1094w"></figure><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/sthmxursey8jbvqpzlz8" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/lbeiaegma0itxsnunz8f 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/cyhdunyljpxh6jkoad0l 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/lvt1drv93iqc9rcw8nuu 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/la7uuuclcg8usbhckr7r 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/i0zftotwb3njvouu5d7e 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/mig6qzpm6kqmvbrtw03k 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/cxmv1ychucmy8shhnh0v 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/q69lsnxdejulik621hc0 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/q7xrq5jnbiumq68jcixj 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/JNjXRBCQJ8RAuqw9n/pp5yeuaisgkuv1wotung 1094w"></figure><p>Speculating about these results, I'm hypothesizing that GPT-2 may organize more around ontological categories rather than pragmatic/social properties. This makes sense to me intuitively: An LLM would be considering a "[Thing] is" prompt to be more like the start of a wikipedia article than the start of a reddit comment about a political opinion on the topic. If this is the case, it makes me wonder if RLHF may be constructing a controversy axis in some cases rather than finding one that already exists. Another possibility, at least for users interacting with LLMs via consumer channels, is that the hedging is just baked in via the system prompt more than anything else.</p><p>To state the significant limitations of this work, certainly I'd start with the n=5 sample for each category being on the small side, and I do plan to replicate this experiment with a larger, and perhaps more rigid, sample. There is also the potential impacts of tokenization confound, and the obvious prompt format constraints. For one example, though the prompts were all the same amount of <i>words</i>, the amount of tokens ranged mostly between 3-5.</p><p>To build on this work, I think my next steps may be repeating the experiment with more prompts, as well as repeating similar testing on different models to see if the theory about the primary axis holds. I'd be especially curious to assess if RLHF has any impact on categorization along this axis.</p><p>Please let me know any thoughts you have, I'm eager to get feedback and discuss.</p><br/><br/><a href="https://www.lesswrong.com/posts/JNjXRBCQJ8RAuqw9n/does-gpt-2-represent-controversy-a-small-mech-interp#comments">Discuss</a>

## Full Content

⚠️ 内容抓取延迟到分析阶段。使用 web_fetch(url) 或 Jina AI 补充。

## Analysis Checklist

- [ ] 阅读全文并提取关键观点
- [ ] 与现有 ZK 笔记建立链接
- [ ] 确定是否转为永久笔记
- [ ] 添加适当的标签和元数据
- [ ] 更新摘要

## Related

- 
