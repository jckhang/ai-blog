---
title: "Why we should expect ruthless sociopath ASI"
source: "ai-alignment RSS"
url: "https://www.alignmentforum.org/posts/ZJZZEuPFKeEdkrRyf/why-we-should-expect-ruthless-sociopath-asi"
date: 2026-02-18T17:28:17.000Z
tags: [rss, ai-alignment]
status: pending-analysis
---

# Why we should expect ruthless sociopath ASI

> Published on February 18, 2026 5:28 PM GMT<br/><br/><h2>The conversation begins</h2><p><strong>(Fictional) Optimist:</strong> So you expect future artificial superintelligence (ASI) “by default”, i.e. in the absence of yet-to-be-invented techniques, to be a ruthless sociopath, happy to lie, cheat, and steal, whenever doing so is selfishly beneficial, and with callous indifference to whether anyone (including its own programmers and users) lives or dies?</p><p><strong>Me:</strong> Yup! (Alas.)</p><p><strong>Optimist:</strong> …Despite all the evidence right in front of our eyes from humans and LLMs.</p><p><strong>Me:</strong> Yup!</p><p><strong>Optimist:</strong> OK, well, I’m here to tell you: that is a very specific and strange thing to expect, especially in the absence of any concrete evidence whatsoever. There’s no reason to expect it. If you think that ruthless sociopathy is the “true core nature of intelligence” or whatever, then&nbsp;<a href="https://www.lesswrong.com/posts/d4HNRdw6z7Xqbnu5E/6-reasons-why-alignment-is-hard-discourse-seems-alien-to?commentId=v2WfXTLaoJuSw9QQG"><u>you should really look at yourself in a mirror and ask yourself where your life went horribly wrong</u></a>.</p><p><strong>Me:</strong> Hmm, I think the “true core nature of intelligence” is above my pay grade. We should probably just talk about the issue at hand, namely future AI algorithms and their properties.</p><p>…But I actually agree with you that ruthless sociopathy is a very specific and strange thing for me to expect.</p><p><strong>Optimist:</strong> Wait, you—what??</p><p><strong>Me:</strong> Yes! Like, if you show me some random thing, there’s a 99.999…% chance that it’s not a ruthless sociopath. Instead it might be, like, a dirt clod. Dirt clods are not ruthless sociopaths, because they’re not intelligent at all.</p><p><strong>Optimist:</strong> Oh c’mon, you know what I mean. I’m not talking about dirt clods. I’m saying, if you pick some random&nbsp;<i>mind</i>,&nbsp; there is no reason at all to expect it to be a ruthless sociopath.</p><p><strong>Me:</strong> How do you “pick some random mind”? Minds don’t just appear out of nowhere.</p><p><strong>Optimist:</strong> Like, a human. Or an AI.</p><p><strong>Me:</strong> Different humans are different to some extent, and different AI algorithms are different to a much, much greater extent. “AI” includes everything from&nbsp;<a href="https://en.wikipedia.org/wiki/A*_search_algorithm"><u>A* search</u></a> to&nbsp;<a href="https://en.wikipedia.org/wiki/MuZero"><u>MuZero</u></a> to LLMs. Is A* search a ruthless sociopath? Well, I mean, it does seem rather maniacally obsessed with graph traversal! Right?</p><p><strong>Optimist:</strong> Haha, very funny. Please stop being annoyingly pedantic. I obviously didn’t mean “AI” in the sense of the academic discipline. I meant, like, AI in the colloquial sense, AI that qualifies as a mind, like LLMs. I’m mainly talking about human minds and LLM “minds”, i.e. all the minds we’ve ever seen in the real world, rather than in sci-fi. And hey, what a coincidence, ≈100% of those minds are&nbsp;<i>not</i> ruthless sociopaths.</p><p><strong>Me:</strong> As it happens, the threat model I’m working on is not LLMs, but rather&nbsp;<a href="https://www.lesswrong.com/s/HzcM2dkCq7fwXBej8"><u>“brain-like” Artificial General Intelligence (AGI)</u></a>, which (from a safety perspective) is more-or-less a type of actor-critic model-based reinforcement learning (RL) agent. LLMs are profoundly different from what I’m working on. Saying that LLMs will be similar to RL-agent AGI because “both are AI” is like saying that LLMs will be similar to the A* search algorithm because “both are AI”, or that a&nbsp;<a href="https://youtu.be/uRP4nAGMYCY?si=uF543PG-v_9DzdwS"><u>frogfish</u></a> will be similar to a human because “both are animals”. They can still be wildly different in every way that matters.</p><h2>Are people worried about LLMs causing doom?</h2><p><strong>Optimist:</strong> OK, but lots of other doomers talk about LLMs causing doom.</p><p><strong>Me:</strong> Well, kinda. I think we need to tease apart two groups of people. Both are sometimes called “doomers”, but one is much more pessimistic than the other. This is very caricatured, but:</p><ul><li>The comparatively-less-pessimistic group (say, P(doom) [probability of human extinction from AI, assuming progress continues] in the 5%–50% range) is a bigger group, and I vaguely associate them with the center-of-gravity of the Effective Altruism movement and&nbsp;<a href="https://www.nytimes.com/2023/07/11/technology/anthropic-ai-claude-chatbot.html"><u>Anthropic employees</u></a>. They definitely do&nbsp;<i>not</i> expect ruthless sociopath ASI as the default path we’re on, absent a technical breakthrough, like I’m arguing for here. At most, they’ll entertain the idea of ruthless sociopath ASI as an odd hypothetical, or as a result of a competitive race-to-the-bottom, or from egregiously careless programmers, or bad actors, etc. They’re probably equally or more concerned about lots of other potential AI problems—AI-assisted bioterrorism, dictatorships, etc.<span class="footnote-reference" data-footnote-reference="" data-footnote-index="1" data-footnote-id="pfdwu3x667" role="doc-noteref" id="fnrefpfdwu3x667"><sup><a href="#fnpfdwu3x667">[1]</a></sup></span></li><li>I’m part of an even more pessimistic group (motto:&nbsp;<a href="https://ifanyonebuildsit.com/"><i><u>If Anyone Builds It, Everyone Dies</u></i></a>), which generally&nbsp;<i>does</i> expect ruthless sociopath ASI as the default path we’re on, absent a technical breakthrough (along with&nbsp;<a href="https://www.lesswrong.com/posts/btHmC88KCZdzimBCM/steve-byrnes-s-shortform?commentId=vzWgcoP3isSBQbDbe"><u>other miracles</u></a>). We tend to think “50% chance that humans will survive continued AI development” is deliriously over-optimistic.</li></ul><p>Anyway, the extra heap of concern in that latter camp is not from the LLMs&nbsp;<i>of today</i> causing near-certain doom, or even the somewhat-better LLMs of tomorrow, but rather the wildly better ASIs of … maybe soon, maybe not, who knows. But even if it’s close in calendar time, and even if it comes out of LLM research, such an ASI would still be systematically different from LLMs as we know them today—</p><p><strong>Optimist:</strong> —a.k.a., you have no evidence—</p><p><strong>Me:</strong> —<a href="https://www.astralcodexten.com/p/the-phrase-no-evidence-is-a-red-flag"><i><u>no evidence either way</u></i></a>, at least no evidence of that type. Anyway, as I was saying, ASI would be systematically different from today’s LLMs because … umm, where do I start …</p><p>…Actually, it would be much easier for me to explain if we&nbsp;<i>start</i> with the ASI threat model that I spend all my time on, and then we can circle back to LLMs afterwards. Is that OK?</p><h2>Positive argument that “brain-like” RL-agent ASI would be a ruthless sociopath</h2><p><strong>Optimist:</strong> Sure. We can pause the discussion of LLMs for a few minutes, and start in your comfort zone of actor-critic model-based RL-agent “brain-like” ASI. Doesn’t really matter anyway: regardless of the exact algorithm, you clearly need some&nbsp;<i>positive</i> reason to believe that this kind of ASI would be a ruthless sociopath. You can’t just unilaterally declare that your weird unprecedented sci-fi belief is the “default”, and push the burden of proof onto people who disagree with you.</p><p><strong>Me:</strong> OK. Maybe a good starting point would be my posts&nbsp;<a href="https://www.lesswrong.com/posts/C5guLAx7ieQoowv3d/lecun-s-a-path-towards-autonomous-machine-intelligence-has-1"><u>LeCun’s ‘A Path Towards Autonomous Machine Intelligence’ has an unsolved technical alignment problem</u></a>, or&nbsp;<a href="https://www.lesswrong.com/posts/TCGgiJAinGgcMEByt/the-era-of-experience-has-an-unsolved-technical-alignment"><u>‘The Era of Experience</u>’<u> has an unsolved technical alignment problem</u></a>.</p><p><strong>Optimist:</strong> I’ve read those, but I’m not seeing how they answer my question. Again, what’s your&nbsp;<i>positive</i> argument for ruthless sociopathy? Lay it on me.</p><p><strong>Me:</strong> Sure. Back at the start of the conversation, I mentioned that random objects like dirt clods are not able to accomplish impressive feats. I didn’t (just) bring up dirt clods to troll you, rather I was laying the groundwork for a key point: If we’re thinking about AI that can autonomously found, grow, and staff innovative companies for years, or autonomously invent new scientific paradigms, then clearly it’s not a “random object”, but rather a thing that is able to accomplish impressive feats. And the question we should be asking is:&nbsp;<i>how does it do that</i>? Those things would be astronomically unlikely to happen if the AI were choosing actions at random. So&nbsp;<strong>there has to be some explanation for how the AI finds actions that accomplish those impressive feats</strong>.<span class="footnote-reference" data-footnote-reference="" data-footnote-index="2" data-footnote-id="qo1n0zw6gza" role="doc-noteref" id="fnrefqo1n0zw6gza"><sup><a href="#fnqo1n0zw6gza">[2]</a></sup></span></p><p>So an explanation has to exist! What is it? I claim there are really only two answers that work in practice.</p><p>The first possible explanation is&nbsp;<strong>consequentialism</strong>: the AI accomplishes impressive feats by (what amounts to) having desires about what winds up happening in the future, and running some search process to find actions that lead to those desires getting fulfilled.&nbsp;<strong>This is the main thing that you get from RL agents, and from model-based planning algorithms.</strong> (My “brain-like AGI” scenario would involve both of those at once.) The whole point of those subfields of AI is: these are algorithms designed to find actions that maximize an objective, by any means available.</p><p>I.e., you get ruthless sociopathic behavior by default.</p><figure class="image image_resized" style="width:90.43%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJZZEuPFKeEdkrRyf/wpuuewhb4bx91w6hrz0f" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJZZEuPFKeEdkrRyf/opsul1xkqits3v2hedix 200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJZZEuPFKeEdkrRyf/nmmxxwoahjvrkyvctndk 400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJZZEuPFKeEdkrRyf/vavnnjkqshzocjnx4cm0 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJZZEuPFKeEdkrRyf/j8knbwedlimc39tuc1rz 800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJZZEuPFKeEdkrRyf/rwub6hi5sxqtcxlpwfrd 1000w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJZZEuPFKeEdkrRyf/atfcysouid0i1xor7ad5 1200w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJZZEuPFKeEdkrRyf/br2f1lykjnr6mfqirjfk 1400w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJZZEuPFKeEdkrRyf/oymunfkg8j6wudv8cdam 1600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJZZEuPFKeEdkrRyf/vutueu7vomkgbk0zi0hb 1800w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJZZEuPFKeEdkrRyf/r3wvyxkiqmtmf5kcpnnb 2000w"><figcaption><a href="https://www.youtube.com/watch?v=00Jjj6oI5fg"><u>Source</u></a></figcaption></figure><p>And this is not just my armchair theorizing. Go find someone who was in AI in the 2010s or earlier, before LLMs took over, and they may well have spent a lot of time building or using RL agents and/or model-based planning algorithms. If so, they’ll tell you, based on their lived experience, that these kinds of algorithms are ruthless by default (when they work at all), unless the programmers go out of their way to make them non-ruthless. See e.g.&nbsp;<a href="https://deepmind.google/blog/specification-gaming-the-flip-side-of-ai-ingenuity/"><u>this 2020 DeepMind blog post on “specification gaming”</u></a>.</p><p>And how would the programmers “go out of their way to make them non-ruthless”? I claim that the answer is not obvious, indeed not even known. See my&nbsp;<a href="https://www.lesswrong.com/posts/C5guLAx7ieQoowv3d/lecun-s-a-path-towards-autonomous-machine-intelligence-has-1"><u>LeCun post</u></a>, and my&nbsp;<a href="https://www.lesswrong.com/posts/TCGgiJAinGgcMEByt/the-era-of-experience-has-an-unsolved-technical-alignment"><u>Silver &amp; Sutton post</u></a>, and more generally my post&nbsp;<a href="https://www.lesswrong.com/posts/FNJF3SoNiwceAQ69W/behaviorist-rl-reward-functions-lead-to-scheming"><u>“‘Behaviorist’ RL reward functions lead to scheming”</u></a> for why obvious approaches to non-ruthlessness won’t work.</p><p>Rather, algorithms in this class are naturally, umm, let’s call them, “ruthless-ifiers”, in the sense that they transmute even innocuous-sounding objectives like “it’s good if the human is happy” into scary-sounding ones like “ruthlessly maximize the probability that the human is happy”, which in turn suggest strategies such as forcibly drugging the human. Likewise, the innocuous-sounding “it’s bad to lie” gets ruthless-ified into “it’s bad to get caught lying”, and so on.</p><p>Of course, evolution&nbsp;<i>did</i> go out of its way to make humans non-ruthless, by endowing us with&nbsp;<a href="https://www.lesswrong.com/posts/kYvbHCDeMTCTE9TAj/neuroscience-of-human-social-instincts-a-sketch"><u>social instincts</u></a>. Maybe future AI programmers will likewise go out of&nbsp;<i>their</i> way to make ASIs non-ruthless? I hope so—but&nbsp;<a href="https://www.lesswrong.com/posts/oxvnREntu82tffkYW/we-need-a-field-of-reward-function-design"><u>we need to figure out how</u></a>.</p><p>To be clear, ruthless consequentialism isn’t always bad. I’m happy for ruthless consequentialist AIs to be&nbsp;<a href="https://en.wikipedia.org/wiki/AlphaZero"><u>playing chess</u></a>,&nbsp;<a href="https://deepmind.google/blog/how-alphachip-transformed-computer-chip-design/"><u>designing chips</u></a>, etc. In principle, I’d even be happy for a ruthless consequentialist AI to be emperor of the universe, creating an awesome future for all—but making that actually happen would be&nbsp;<i>super</i> dangerous for&nbsp;<a href="https://www.lesswrong.com/posts/wucncPjud27mLWZzQ/intro-to-brain-like-agi-safety-10-the-technical-alignment"><u>lots of reasons</u></a> (e.g. you might need to operationalize “creating an awesome future for all” in a loophole-free way; see also&nbsp;<a href="https://www.lesswrong.com/posts/wucncPjud27mLWZzQ/intro-to-brain-like-agi-safety-10-the-technical-alignment#10_3_3_1__The_usual_agent_debugging_loop___and_its_future_catastrophic_breakdown"><u>“‘The usual agent debugging loop’, and its future catastrophic breakdown”</u></a>).</p><p>…So that’s consequentialism, one possible answer for how an AI might accomplish impressive feats, and it’s an answer that brings in ruthlessness by default.</p><h2>Circling back LLMs: imitative learning vs ASI</h2><p>…And then there’s a second, different possible answer to how an AI might accomplish impressive feats:&nbsp;<strong>imitative learning</strong> from humans. You train an AI to predict what actions a skilled human would take in many different contexts, and then have the AI take that same action itself. I claim that&nbsp;<strong>LLMs</strong> get their impressive capabilities almost entirely from imitative learning.<span class="footnote-reference" data-footnote-reference="" data-footnote-index="3" data-footnote-id="0j4qx71vu1gg" role="doc-noteref" id="fnref0j4qx71vu1gg"><sup><a href="#fn0j4qx71vu1gg">[3]</a></sup></span><strong>&nbsp;</strong>By contrast, “true” imitative learning is entirely absent (and impossible) in humans and animals.<span class="footnote-reference" data-footnote-reference="" data-footnote-index="4" data-footnote-id="g0iv4wihdic" role="doc-noteref" id="fnrefg0iv4wihdic"><sup><a href="#fng0iv4wihdic">[4]</a></sup></span></p><p>Imitative-learning AIs do&nbsp;<i>not</i> have ruthless sociopathy by default, because of course the thing they’re imitating is non-ruthless humans.<span class="footnote-reference" data-footnote-reference="" data-footnote-index="5" data-footnote-id="lovljvyg1td" role="doc-noteref" id="fnreflovljvyg1td"><sup><a href="#fnlovljvyg1td">[5]</a></sup></span></p><p><strong>Optimist:</strong> Huh … Wait … So you’re an optimist about superintelligence (ASI) being non-ruthless, as long as people stick to LLMs?</p><p><strong>Me:</strong> Alas, no. I think that the full power of consequentialism is super dangerous by default,&nbsp;<i>and</i> I think that the full power of consequentialism is the only way to get ASI, and so AI researchers are going to keep working until they eventually learn to fully tap that power.</p><p>In other words, I see a disjunction:</p><ul><li>EITHER, LLMs will always get their powers primarily from imitative learning, as I claim they do today—in which case they will never be able to figure things out way beyond the human-created training data, and will thus never reach ASI. And then eventually we’ll get ASI via a&nbsp;<i>different</i> AI paradigm, one that&nbsp;<i>can</i> rocket arbitrarily far past any human data. And that paradigm will have to draw its powers from consequentialism, which brings in ruthlessness-by-default.</li><li>OR, someone will figure out how to get LLMs themselves to rocket arbitrarily far past human training data and into ASI. But the only way to do that is to somehow modify LLMs to draw on the full powers of consequentialism. In which case, again, we get ruthlessness-by-default.</li></ul><figure class="image image_resized" style="width:72.11%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJZZEuPFKeEdkrRyf/qpukod1h632c86hi5wqw" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJZZEuPFKeEdkrRyf/foylvrj971vofoaxfced 110w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJZZEuPFKeEdkrRyf/rvxodtefxakhcm5h5yd5 220w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJZZEuPFKeEdkrRyf/gwi13jmibnf8e4wgarz2 330w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJZZEuPFKeEdkrRyf/cftcelhw99xxjj9e8sok 440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJZZEuPFKeEdkrRyf/mrkst2sg6z6cx7hdwyuc 550w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJZZEuPFKeEdkrRyf/madsskqwknt6wkfs3uaw 660w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJZZEuPFKeEdkrRyf/pgzdu9skvaghoddclh2g 770w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJZZEuPFKeEdkrRyf/uwpogazlux50lcfoplug 880w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJZZEuPFKeEdkrRyf/nx7dhlldayxio8lbrrqg 990w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZJZZEuPFKeEdkrRyf/bwf4qoi0oovrobsjfkwj 1020w"><figcaption><a href="https://www.youtube.com/watch?v=aFW6FGV_tls"><u>Source</u></a></figcaption></figure><p>For what it’s worth, I happen to expect that ASI will come from the former (future paradigm shift) rather than the latter (LLM modifications). But it hardly matters in this context.</p><p><strong>Optimist:</strong> I dunno, if you’re willing to concede that LLMs today are not maximally ruthless, well, LLMs today don’t seem that far from superintelligence. I mean, humans don’t “rocket arbitrarily far past any training data” either. They usually do things that have been done before, or at most (for world experts on the bleeding edge) go just one little step beyond it. LLMs can do both, right?</p><p><strong>Me:</strong> Yes, but humans&nbsp;<i>collectively and over time</i> can get way, way, way beyond our training data. We’re still using the same brain design that we were using in Pleistocene Africa. Between then and now, there were no angels who dropped training data from the heavens, but humans nevertheless invented language, science, technology, industry, culture, and everything else in the $100T global economy&nbsp;<i>entirely from scratch</i>. We did it all by ourselves, by our own bootstraps, ultimately via the power of consequentialism, as implemented in the RL and model-based planning algorithms in our brains.</p><p>(See&nbsp;<a href="https://www.lesswrong.com/posts/2yLyT6kB7BQvTfEuZ/sharp-left-turn-discourse-an-opinionated-review"><u>“Sharp Left Turn” discourse: An opinionated review</u></a>.)</p><p>By the same token, if humanity survives another 1000 years, we will invent wildly new scientific paradigms, build wildly new industries and ways of thinking, etc.</p><p><a href="https://www.lesswrong.com/posts/xJWBofhLQjf3KmRgg/four-ways-learning-econ-makes-people-dumber-re-future-ai"><u>There’s a quadrillion-dollar market</u></a> for AIs that can likewise do that kind of thing, as humans can. If the LLMs of today don’t pass that bar (and they don’t), then I expect that, sooner or later, either someone will figure out how to get LLMs to pass that bar, or else someone will invent a new non-LLM AI paradigm that passes that bar. Either way, imitative learning is out, consequentialism is in, and we get ruthless sociopath ASIs by default, in the absence of yet-to-be-invented theoretical advances in technical alignment. (And then everyone dies.)</p><p><i>Thanks Jeremy Gillen, Seth Herd, and Justis Mills for critical comments on earlier drafts.</i></p><ol class="footnote-section footnotes" data-footnote-section="" role="doc-endnotes"><li class="footnote-item" data-footnote-item="" data-footnote-index="1" data-footnote-id="pfdwu3x667" role="doc-endnote" id="fnpfdwu3x667"><span class="footnote-back-link" data-footnote-back-link="" data-footnote-id="pfdwu3x667"><sup><strong><a href="#fnrefpfdwu3x667">^</a></strong></sup></span><div class="footnote-content" data-footnote-content=""><p>We should definitely also be thinking about these other potential problems, don’t get me wrong!</p></div></li><li class="footnote-item" data-footnote-item="" data-footnote-index="2" data-footnote-id="qo1n0zw6gza" role="doc-endnote" id="fnqo1n0zw6gza"><span class="footnote-back-link" data-footnote-back-link="" data-footnote-id="qo1n0zw6gza"><sup><strong><a href="#fnrefqo1n0zw6gza">^</a></strong></sup></span><div class="footnote-content" data-footnote-content=""><p>Related: the<a href="https://www.lesswrong.com/posts/k6EPphHiBH4WWYFCj/gazp-vs-glut"> so-called&nbsp;<u>“Follow-the-Improbability Game”</u></a>.</p></div></li><li class="footnote-item" data-footnote-item="" data-footnote-index="3" data-footnote-id="0j4qx71vu1gg" role="doc-endnote" id="fn0j4qx71vu1gg"><span class="footnote-back-link" data-footnote-back-link="" data-footnote-id="0j4qx71vu1gg"><sup><strong><a href="#fnref0j4qx71vu1gg">^</a></strong></sup></span><div class="footnote-content" data-footnote-content=""><p>Details: “imitative learning” describes LLM pretraining, but not posttraining; my claim is that LLM capabilities come almost entirely from the former, not the latter. That’s not obvious, but I argue for it in&nbsp;<a href="https://www.lesswrong.com/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard#2_3_3_To_what_extent_should_we_think_of_LLMs_as_imitating_"><u>“Foom &amp; Doom” §2.3.3</u></a>, and see also a couple papers downplaying the role of RLVR (<a href="https://aakaran.github.io/reasoning_with_sampling/"><u>Karan &amp; Du 2025</u></a>,&nbsp;<a href="https://arxiv.org/abs/2510.07364"><u>Venhoff et al. 2025</u></a>), along with&nbsp;<a href="https://www.beren.io/2025-08-02-Most-Algorithmic-Progress-is-Data-Progress/"><u>“Most Algorithmic Progress is Data Progress”</u></a> by Beren Millidge.</p></div></li><li class="footnote-item" data-footnote-item="" data-footnote-index="4" data-footnote-id="g0iv4wihdic" role="doc-endnote" id="fng0iv4wihdic"><span class="footnote-back-link" data-footnote-back-link="" data-footnote-id="g0iv4wihdic"><sup><strong><a href="#fnrefg0iv4wihdic">^</a></strong></sup></span><div class="footnote-content" data-footnote-content=""><p>E.g. if my brain is predicting what someone else will say, that’s related to auditory inputs, and if my brain is speaking, that involves motor-control commands going to my larynx etc. There is no straightforward mechanical translation from one to the other, analogous to the straightforward mechanical translation from “predict the next token” to “output the next token” in LLM pretraining. More in&nbsp;<a href="https://www.lesswrong.com/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard#2_3_2_LLM_pretraining_magically_transmutes_observations_into_behavior__in_a_way_that_is_profoundly_disanalogous_to_how_brains_work"><u>“Foom &amp; Doom” §2.3.2</u></a>.</p></div></li><li class="footnote-item" data-footnote-item="" data-footnote-index="5" data-footnote-id="lovljvyg1td" role="doc-endnote" id="fnlovljvyg1td"><span class="footnote-back-link" data-footnote-back-link="" data-footnote-id="lovljvyg1td"><sup><strong><a href="#fnreflovljvyg1td">^</a></strong></sup></span><div class="footnote-content" data-footnote-content=""><p>See&nbsp;<a href="https://www.lesswrong.com/posts/nH4c3Q9t9F3nJ7y8W/gpts-are-predictors-not-imitators"><u>GPTs are Predictors, not Imitators</u></a> for an even-more-pessimistic-than-me counterargument, and&nbsp;<a href="https://www.lesswrong.com/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard#2_3_3_To_what_extent_should_we_think_of_LLMs_as_imitating_"><u>“Foom &amp; Doom” §2.3.3</u></a> for why I don’t buy that counterargument.</p></div></li></ol><br/><br/><a href="https://www.alignmentforum.org/posts/ZJZZEuPFKeEdkrRyf/why-we-should-expect-ruthless-sociopath-asi#comments">Discuss</a>

## Full Content

⚠️ 内容抓取延迟到分析阶段。使用 web_fetch(url) 或 Jina AI 补充。

## Analysis Checklist

- [ ] 阅读全文并提取关键观点
- [ ] 与现有 ZK 笔记建立链接
- [ ] 确定是否转为永久笔记
- [ ] 添加适当的标签和元数据
- [ ] 更新摘要

## Related

- 
