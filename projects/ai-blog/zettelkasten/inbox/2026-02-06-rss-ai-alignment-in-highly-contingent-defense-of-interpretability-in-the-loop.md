---
title: "In (highly contingent!) defense of interpretability-in-the-loop ML training"
source: "ai-alignment RSS"
url: "https://www.alignmentforum.org/posts/ArXAyzHkidxwoeZsL/in-highly-contingent-defense-of-interpretability-in-the-loop"
date: 2026-02-06T16:32:27.000Z
tags: [rss, ai-alignment]
status: pending-analysis
---

# In (highly contingent!) defense of interpretability-in-the-loop ML training

> Published on February 6, 2026 4:32 PM GMT<br/><br/><p>Let’s call<i> “interpretability-in-the-loop training”</i> the idea of running a learning algorithm that involves an inscrutable trained model, and there’s some kind of interpretability system feeding into the loss function / reward function.</p><p>Interpretability-in-the-loop training has a very bad rap (and rightly so). Here’s&nbsp;<a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities"><u>Yudkowsky 2022</u></a>:</p><blockquote><p>When you explicitly optimize against a detector of unaligned thoughts, you're partially optimizing for more aligned thoughts, and partially optimizing for unaligned thoughts that are harder to detect.&nbsp; Optimizing against an interpreted thought optimizes against interpretability.</p></blockquote><p>Or&nbsp;<a href="https://www.lesswrong.com/posts/mpmsK8KKysgSKDm2T/the-most-forbidden-technique"><u>Zvi 2025</u></a>:</p><blockquote><p><a href="https://thezvi.substack.com/i/145384938/the-art-of-the-jailbreak"><u>The Most Forbidden Technique</u></a> is training an AI using interpretability techniques.</p><p>An AI produces a final output [X] via some method [M]. You can analyze [M] using technique [T], to learn what the AI is up to. You could train on that. Never do that.</p><p>You train on [X]. Only [X]. Never [M], never [T].</p><p>Why? Because [T] is how you figure out when the model is misbehaving.</p><p>If you train on [T], you are training the AI to obfuscate its thinking, and defeat [T]. You will rapidly lose your ability to know what is going on, in exactly the ways you most need to know what is going on.</p><p>Those bits of optimization pressure from [T] are precious. Use them wisely.</p></blockquote><p>This is a simple argument, and I think it’s 100% right.</p><p><i>But…</i></p><p>Consider compassion in the human brain. <a href="https://www.lesswrong.com/posts/KuBiv9cCbZ6ALjHFw/social-drives-1-sympathy-reward-from-compassion-to">I claim</a> that we have an innate reward function that triggers not just when I see that my friend is happy or suffering, but also when I&nbsp;<i>believe</i> that my friend is happy or suffering, even if the friend is far away. So the human brain reward can evidently get triggered by specific activations inside my inscrutable learned world-model.</p><p>Thus, I claim that the human brain incorporates a form of interpretability-in-the-loop RL training.</p><p>Inspired by that example, I have long been an advocate for studying whether and how one might use interpretability-in-the-loop training for aligned AGI. See for example&nbsp;<a href="https://www.lesswrong.com/posts/xw8P8H4TRaTQHJnoP/reward-function-design-a-starter-pack"><u>Reward Function Design: a starter pack</u></a> sections 1, 4, and 5.</p><p>My goal in the post is to briefly summarize how I reconcile the arguments at the top with my endorsement of this kind of research program.</p><h1>My overall position</h1><ul><li>Yudkowsky &amp; Zvi are correct that straightforward interpretability-in-the-loop training of LLMs is almost definitely a very bad idea.<ul><li>…Unless interpretability someday develops to such a refined point that it’s adversarially robust (i.e., we understand the model <i>so well</i> that problematic thoughts have nowhere to hide from the interpretability tools). But that sure seems like a long-shot.</li></ul></li><li>There exists a certain <i>brain-like</i> version of interpretability-in-the-loop RL training that does not suffer from this very obvious failure mode<ul><li>(…although it might or might not fail for other reasons!)</li></ul></li><li>And there’s a straightforward explanation of <i>exactly how</i> it avoids this obvious failure mode.</li></ul><p>The rest of this post will present this explanation:</p><h1>How the brain-like version of interpretability-in-the-loop training avoids the obvious failure mode</h1><p>The human brain has beliefs and desires. They are different. It’s possible to want something without expecting it, and it’s possible to expect something without wanting it. This should be obvious common sense to everyone,&nbsp;<a href="https://www.lesswrong.com/posts/MArdnet7pwgALaeKs/why-i-m-not-into-the-free-energy-principle"><u>unless your common sense has been crowded out by “active inference” nonsense</u></a>.</p><p>Beliefs and desires are stored in different parts of the brain, and updated in different ways. (<a href="https://www.lesswrong.com/posts/bnnKGSCHJghAvqPjS/foom-and-doom-2-technical-alignment-is-hard#2_3_2_LLM_pretraining_magically_transmutes_observations_into_behavior__in_a_way_that_is_profoundly_disanalogous_to_how_brains_work">This is a huge disanalogy between LLMs and brains.</a>)</p><p>As an oversimplified toy model, I suggest to think of desires as a learned linear functional on beliefs (see my&nbsp;<a href="https://www.lesswrong.com/s/6uDBPacS6zDipqbZ9/p/SqgRtCwueovvwxpDQ#2_4_1_Side_note__Valence_as_a__roughly__linear_function_over_compositional_thought_pieces"><u>Valence series §2.4.1</u></a>). I.e. “desires” constitute a map whose&nbsp;<i>input</i> is some thought / plan / etc. (over on the belief side), and whose&nbsp;<i>output</i> is a numerical score indicating whether that thought / plan / etc. is good (if the score is positive) or bad (if it’s negative).</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ArXAyzHkidxwoeZsL/gwwwx5undurzhxu7cpbm" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ArXAyzHkidxwoeZsL/ackbqbaezbvj9d2subvw 120w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ArXAyzHkidxwoeZsL/c1l5nxe0czj3pbbksy3p 240w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ArXAyzHkidxwoeZsL/dw0htiv0alnjlke1omyj 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ArXAyzHkidxwoeZsL/lrb4xofi8rkdzffkumth 480w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ArXAyzHkidxwoeZsL/y4wz9kydkxweqpmxx93z 600w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ArXAyzHkidxwoeZsL/gzrfwht6nqpok2dlwkx8 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ArXAyzHkidxwoeZsL/ddswnoxsyl2bgrodifxj 840w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ArXAyzHkidxwoeZsL/mkht0ynqjrufpvrp0ca7 960w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ArXAyzHkidxwoeZsL/ox9cdbwudoxdwgmdzojn 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ArXAyzHkidxwoeZsL/rao2g020jaiak1jrqnv2 1129w"></figure><p>Anyway, the important point is that these two boxes are updated in different ways. Let’s expand the diagram to include the different updating systems, and how interpretability-in-the-loop training fits in:</p><figure class="image"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ArXAyzHkidxwoeZsL/pkdno6fyyvliaygjlbfh" srcset="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ArXAyzHkidxwoeZsL/d5drbgolv0sa2gi81hxk 180w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ArXAyzHkidxwoeZsL/zrpoqiqynceay7t5rn0t 360w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ArXAyzHkidxwoeZsL/hz6kfctcbljtbhvdvpe1 540w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ArXAyzHkidxwoeZsL/lf08zmnyphhgmouwqn6l 720w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ArXAyzHkidxwoeZsL/u4fmjcflgukc7cfmhpu9 900w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ArXAyzHkidxwoeZsL/nhwfbefxgijvvseioppd 1080w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ArXAyzHkidxwoeZsL/znsi2us0liehwwiyb2y1 1260w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ArXAyzHkidxwoeZsL/azwbklr4ompoprbagxvv 1440w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ArXAyzHkidxwoeZsL/fgfwsnjkyvwazp3ptv3m 1620w, https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ArXAyzHkidxwoeZsL/fvcelxb7umwfnoysnylv 1719w"><figcaption>The red arrow indicates that there’s some system (any system you like) that looks at the inscrutable activation state in the “Beliefs” box, and spits out one or more numerical metrics related to that state. Those numbers then go to the reward function, where they help determine the reward signals.</figcaption></figure><p>The interpretability data is changing the reward signals, but the reward signals are not directly changing the belief box that the interpretability system is querying.</p><p>That means:<strong> The loop doesn’t close. This interpretability system is not creating any gradient that directly undermines its own faithfulness.</strong><span class="footnote-reference" data-footnote-reference="" data-footnote-index="1" data-footnote-id="wf4b4offf5" role="doc-noteref" id="fnrefwf4b4offf5"><sup><a href="#fnwf4b4offf5">[1]</a></sup></span></p><p>…So that’s how this brain-like setup avoids the obvious failure mode of interpretability-in-the-loop that Yudkowsky &amp; Zvi were talking about at the top.</p><h2>Things can still go wrong in more subtle and indirect ways</h2><p>…Or at least, it avoids the most straightforward manifestation of that problem. There are more subtle things that might go wrong. I have a high-level generic discussion in <a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs#3_3_Motivated_reasoning___thinking___observing__including_confirmation_bias"><u>Valence series §3.3</u></a>, where I point out that there exist indirect pathways through this diagram, and discuss how they can cause problems:</p><figure class="image image_resized" style="width:85.09%"><img src="https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/rM8DwFKZM4eB7i2p8/u5y0mlmde5fn18d8vsxk"><figcaption>Diagram copied from <a href="https://www.lesswrong.com/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs">Valence series §3</a>&nbsp;</figcaption></figure><p>And these kinds of problems can indeed pop up in the context of compassion and other interpretability-in-the-loop human social instincts. The result is that human social instincts that <i>on paper</i> might look robustly prosocial, are in fact not so robustly prosocial in the real (human) world. See my&nbsp;<a href="https://www.lesswrong.com/posts/KuBiv9cCbZ6ALjHFw/social-drives-1-sympathy-reward-from-compassion-to#4_1_False_negatives__e_g__dehumanization_"><u>Sympathy Reward post §4.1</u></a> and&nbsp;<a href="https://www.lesswrong.com/posts/fPxgFHfs5yHzYqgG7/social-drives-2-approval-reward-from-norm-enforcement-to#6__How_robust_are_the_effects_of_Approval_Reward_"><u>Approval Reward post §6</u></a> for lots of everyday examples.</p><p>So the upshot is: I don’t think the brain-like version of interpretability-in-the-loop RL training is a panacea for aligned ASI, and I’m open-minded to the possibility that it’s just not a viable approach at all. But it’s at least a not-<i>obviously</i>-doomed research direction, and merits more study.&nbsp;</p><ol class="footnote-section footnotes" data-footnote-section="" role="doc-endnotes"><li class="footnote-item" data-footnote-item="" data-footnote-index="1" data-footnote-id="wf4b4offf5" role="doc-endnote" id="fnwf4b4offf5"><span class="footnote-back-link" data-footnote-back-link="" data-footnote-id="wf4b4offf5"><sup><strong><a href="#fnrefwf4b4offf5">^</a></strong></sup></span><div class="footnote-content" data-footnote-content=""><p><strong>Added 2026-02-13:</strong> Actually, oops, even this sentence is a bit oversimplified. E.g. here’s a scenario. There’s an anti-deception probe connected to the reward function, and the “beliefs” box has two preexisting plans / actions: (A) a “be deceptive in a way that triggers the alarm” plan / action in the “beliefs” box, and (B) a “be deceptive in a way that doesn’t trigger the alarm” plan / action.</p><p>Now, the good news is that there wouldn’t be a predictive learning gradient that would push towards (B), nor one that would create (B) if (B) didn’t already exist. But the bad news is, there is a kind of <i>policy</i> gradient that would ensure that, if (B) occurs by random happenstance, then the system will update to repeat (B) more often in the future. (Or worse, there could be a partway-to-(B) plan / action that’s somewhat rewarded, etc., and then the system may hill-climb to (B).)</p><p>I still think this setup is much <i>less</i> obviously doomed than the LLM case at the top, because we have all this learned structure in the “beliefs” box that the reward function is not allowed to manipulate directly. Again, for example, if (B) doesn’t already exist within the web of constraints that constitutes the “beliefs” box, this system won’t (directly) create it.</p><p><i>[Thanks Rhys Gould for discussion of this point.]</i></p></div></li></ol><br/><br/><a href="https://www.alignmentforum.org/posts/ArXAyzHkidxwoeZsL/in-highly-contingent-defense-of-interpretability-in-the-loop#comments">Discuss</a>

## Full Content

⚠️ 内容抓取延迟到分析阶段。使用 web_fetch(url) 或 Jina AI 补充。

## Analysis Checklist

- [ ] 阅读全文并提取关键观点
- [ ] 与现有 ZK 笔记建立链接
- [ ] 确定是否转为永久笔记
- [ ] 添加适当的标签和元数据
- [ ] 更新摘要

## Related

- 
