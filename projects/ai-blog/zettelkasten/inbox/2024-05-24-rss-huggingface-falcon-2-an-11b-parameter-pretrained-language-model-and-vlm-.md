---
title: "Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages"
source: "huggingface RSS"
url: "https://huggingface.co/blog/falcon2-11b"
date: 2024-05-24T00:00:00.000Z
tags: [rss, huggingface]
status: pending-analysis
---

# Falcon 2: An 11B parameter pretrained language model and VLM, trained on over 5000B tokens and 11 languages

> No description available.

## Full Content

⚠️ 内容抓取延迟到分析阶段。使用 web_fetch(url) 或 Jina AI 补充。

## Analysis Checklist

- [ ] 阅读全文并提取关键观点
- [ ] 与现有 ZK 笔记建立链接
- [ ] 确定是否转为永久笔记
- [ ] 添加适当的标签和元数据
- [ ] 更新摘要

## Related

- 
