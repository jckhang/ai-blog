---
title: "Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation"
source: "arxiv-cs-ai RSS"
url: "https://arxiv.org/abs/2602.16727"
date: 2026-02-20T05:00:00.000Z
tags: [rss, arxiv-cs-ai]
status: pending-analysis
---

# Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation

> arXiv:2602.16727v1 Announce Type: new 
Abstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.

## Full Content

⚠️ Full content not available (fetch failed).

## Analysis Checklist

- [ ] 阅读全文并提取关键观点
- [ ] 与现有 ZK 笔记建立链接 [[...]]
- [ ] 确定是否转为永久笔记
- [ ] 添加适当的标签和元数据
- [ ] 更新摘要

## Related

- 
